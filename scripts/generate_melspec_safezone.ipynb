{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BirdCLEF 2025 Data Preprocessing Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:10:40.81525Z",
     "iopub.status.busy": "2025-03-17T13:10:40.814873Z",
     "iopub.status.idle": "2025-03-17T13:10:45.829114Z",
     "shell.execute_reply": "2025-03-17T13:10:45.828024Z",
     "shell.execute_reply.started": "2025-03-17T13:10:40.815215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from module import config_lib, utils_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    def __init__(self, kaggle_notebook=False, debug=False):\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.PROCESSED_DIR = \"\"\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.model_path = '/kaggle/input/birdclef-2025-0330'\n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            self.train_csv = '../data/raw/train.csv'\n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.models_dir = \"../models/\" # å…¨modelã®ä¿å­˜å…ˆ\n",
    "            self.model_path = self.models_dir # å„ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆï¼å­¦ç¿’æ™‚ã«å‹•çš„ã«å¤‰æ›´ï¼\n",
    "            self.RAW_DIR = '../data/raw/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.TARGET_DURATION = 5 # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæ™‚ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        self.N_FFT = 1024\n",
    "        self.HOP_LENGTH = 16\n",
    "        self.N_MELS = 148\n",
    "        self.FMIN = 20\n",
    "        self.FMAX = 16000\n",
    "        self.N_MAX = 50 if self.debug else None        \n",
    "        self.N_JOBS = 16  # ä¸¦åˆ—å‡¦ç†ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•° 16ãã‚‰ã„ã§ã„ã„\n",
    "        self.N_JOBS_DURATION = 47\n",
    "        \n",
    "        self.LOAD_ENGINE = 'torchaudio'  # librosa or torchaudio\n",
    "        self.SKIP_RESIZE = False  # resizeã—ãªã„ãªã‚‰True\n",
    "        self.seed = 42\n",
    "        self.n_fold = 5\n",
    "        self.num_rare_samples = 10 # ã“ã‚Œä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã®speciesã¯rare speciesã¨ã—ã¦æ‰±ã†\n",
    "        self.is_crop_aug = False\n",
    "        self.num_prune_samples = 1000 # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ã“ã‚Œä»¥ä¸‹ã«prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:15:45.762845Z",
     "iopub.status.busy": "2025-03-17T13:15:45.762471Z",
     "iopub.status.idle": "2025-03-17T13:15:45.768405Z",
     "shell.execute_reply": "2025-03-17T13:15:45.766979Z",
     "shell.execute_reply.started": "2025-03-17T13:15:45.762812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = DatasetConfig(kaggle_notebook=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_lib.set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:16:15.414418Z",
     "iopub.status.busy": "2025-03-17T13:16:15.414035Z",
     "iopub.status.idle": "2025-03-17T13:16:15.55526Z",
     "shell.execute_reply": "2025-03-17T13:16:15.553984Z",
     "shell.execute_reply.started": "2025-03-17T13:16:15.414356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: OFF\n",
      "Max samples to process: ALL\n",
      "Loading taxonomy data...\n",
      "Loading training metadata...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Debug mode: {'ON' if config.debug else 'OFF'}\")\n",
    "print(f\"Max samples to process: {config.N_MAX if config.N_MAX is not None else 'ALL'}\")\n",
    "\n",
    "print(\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(f'{config.RAW_DIR}/taxonomy.csv')\n",
    "species_class_map = dict(zip(taxonomy_df['primary_label'], taxonomy_df['class_name']))\n",
    "\n",
    "print(\"Loading training metadata...\")\n",
    "train_df = pd.read_csv(f'{config.RAW_DIR}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:16:43.589257Z",
     "iopub.status.busy": "2025-03-17T13:16:43.588879Z",
     "iopub.status.idle": "2025-03-17T13:16:43.644396Z",
     "shell.execute_reply": "2025-03-17T13:16:43.643479Z",
     "shell.execute_reply.started": "2025-03-17T13:16:43.589225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206 unique species\n",
      "Total samples to process: 28564 out of 28564 available\n",
      "Samples by class:\n",
      "class\n",
      "Aves        27648\n",
      "Amphibia      583\n",
      "Mammalia      178\n",
      "Insecta       155\n",
      "Name: count, dtype: int64\n",
      "âœ… Added 'duration_sec'. Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "label_list = sorted(train_df['primary_label'].unique())\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))\n",
    "\n",
    "print(f'Found {len(label_list)} unique species')\n",
    "working_df = train_df.copy()\n",
    "working_df['target'] = working_df.primary_label.map(label2id)\n",
    "working_df['filepath'] = config.RAW_DIR + '/train_audio/' + working_df.filename\n",
    "working_df['samplename'] = working_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "working_df['class'] = working_df.primary_label.map(lambda x: species_class_map.get(x, 'Unknown'))\n",
    "working_df[\"crop_strategy\"] = \"center\"\n",
    "total_samples = min(len(working_df), config.N_MAX or len(working_df))\n",
    "print(f'Total samples to process: {total_samples} out of {len(working_df)} available')\n",
    "print(f'Samples by class:')\n",
    "print(working_df['class'].value_counts())\n",
    "\n",
    "# éŸ³æºã®é•·ã•ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¿½åŠ ï¼\n",
    "duration_df = pd.read_csv(\"../data/processed/train_duration.csv\")\n",
    "working_df = working_df.merge(duration_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "missing = working_df[\"duration_sec\"].isna().sum()\n",
    "print(f\"âœ… Added 'duration_sec'. Missing values: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å…¨éŸ³æºã®é•·ã•ã‚’è¨ˆç®—\n",
    "# def get_duration(filepath, sr):\n",
    "#     try:\n",
    "#         audio, _ = librosa.load(filepath, sr=sr)\n",
    "#         return len(audio) / sr\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Could not load {filepath}: {e}\")\n",
    "#         return np.nan\n",
    "\n",
    "# print(\"ğŸ”„ Calculating durations with parallel processing...\")\n",
    "\n",
    "# # tqdm å¯¾å¿œ\n",
    "# filepaths = working_df['filepath'].tolist()\n",
    "# durations = Parallel(n_jobs=config.N_JOBS_DURATION)(\n",
    "#     delayed(get_duration)(fp, config.FS) for fp in tqdm(filepaths)\n",
    "# )\n",
    "\n",
    "# working_df['duration_sec'] = durations\n",
    "# print(\"âœ… Added 'duration_sec' column to working_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[\"valid_start_sec\"] = 0\n",
    "working_df[\"valid_end_sec\"] = working_df[\"duration_sec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹å‹•ã§äººã®å£°é™¤å»\n",
    "\n",
    "# 4. ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã® valid_start_sec ã‚’å¤‰æ›´\n",
    "# ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€åˆã«ã‚¹ãƒšã‚¤ãƒ³èªãŒå«ã¾ã‚Œã‚‹ï¼‰\n",
    "spanish_intro_filenames = [\n",
    "    '50186/CSA28885.ogg',\n",
    "    '52884/CSA14875.ogg'\n",
    "]\n",
    "# valid_start_sec ã‚’ 4.0 ã«å¤‰æ›´\n",
    "working_df.loc[\n",
    "    working_df['filename'].isin(spanish_intro_filenames),\n",
    "    'valid_start_sec'\n",
    "] = 4.0\n",
    "\n",
    "\n",
    "# é€”ä¸­ã§äººã®å£°ã®ã¿ã«ãªã‚‹ã®ã§é™¤å»\n",
    "voice_only_ranges = {\n",
    "    '476537/CSA35459.ogg': 134,  # 2åˆ†14ç§’ = 134ç§’\n",
    "    '476537/CSA35461.ogg': 259,  # 4åˆ†19ç§’ = 259ç§’\n",
    "}\n",
    "for fname, end_sec in voice_only_ranges.items():\n",
    "    working_df.loc[\n",
    "        working_df['filename'] == fname,\n",
    "        'valid_end_sec'\n",
    "    ] = end_sec\n",
    "\n",
    "\n",
    "# Eliana Barona- CortÃ©sã€€ã®éŸ³æºï¼è©±ã—ã¦ã„ã‚‹éƒ¨åˆ†ï¼ã„ã‚‰ãªã„éƒ¨åˆ†\n",
    "# 24292/CSA34649.ogg 2min8ä»¥é™\n",
    "# 24292/CSA34651.ogg 1min33ä»¥é™\n",
    "# 50186/CSA34622.ogg 21sä»¥é™\n",
    "# 50186/CSA34678.ogg 43sä»¥é™\n",
    "voice_only_ranges = {\n",
    "    '24292/CSA34649.ogg': 128,   # 2åˆ†8ç§’ = 128ç§’\n",
    "    '24292/CSA34651.ogg': 93,    # 1åˆ†33ç§’ = 93ç§’\n",
    "    '50186/CSA34622.ogg': 21,    # 21ç§’\n",
    "    '50186/CSA34678.ogg': 43,    # 43ç§’\n",
    "}\n",
    "for fname, end_sec in voice_only_ranges.items():\n",
    "    working_df.loc[\n",
    "        working_df['filename'] == fname,\n",
    "        'valid_end_sec'\n",
    "    ] = end_sec\n",
    "\n",
    "# Alexandra Butrago-Cardona ã®éŸ³æºãƒã‚§ãƒƒã‚¯\n",
    "# è©±ã—ã¦ã„ã‚‹éƒ¨åˆ†ï¼ã„ã‚‰ãªã„éƒ¨åˆ†\n",
    "# 24292/CSA35021.ogg 36sä»¥é™\n",
    "# 52884/CSA34947.ogg 13sä»¥é™\n",
    "voice_only_ranges = {\n",
    "    '24292/CSA35021.ogg': 36,    # 36ç§’\n",
    "    '52884/CSA34947.ogg': 13,     # 13ç§’\n",
    "}\n",
    "for fname, end_sec in voice_only_ranges.items():\n",
    "    working_df.loc[\n",
    "        working_df['filename'] == fname,\n",
    "        'valid_end_sec'\n",
    "    ] = end_sec\n",
    "\n",
    "\n",
    "# Fabio A. Sarria-S ã®éŸ³å£°ã¯ 0ã€œ7ç§’ ã ã‘ä½¿ç”¨å¯èƒ½ã«è¨­å®šï¼å¾ŒåŠã¯ãŸã ã®èª¬æ˜ãªã®ã§\n",
    "fabio_filenames = train_df.loc[\n",
    "    train_df['author'] == \"Fabio A. Sarria-S\", 'filename'\n",
    "].tolist()\n",
    "# \n",
    "working_df.loc[\n",
    "    working_df['filename'].isin(fabio_filenames), 'valid_end_sec'\n",
    "] = 7.0\n",
    "\n",
    "#  Fabioã®è§£èª¬ã§ï¼Œå¿…ãšã—ã‚‚7secã§ã¯ãªã„ã‚‚ã®\n",
    "fabio_override = {\n",
    "    \"48124/CSA36346.ogg\": 24.0,\n",
    "    \"52884/CSA36344.ogg\": 55.0,\n",
    "    \"52884/CSA36342.ogg\": 14.0,  # â† è¿½åŠ åˆ†\n",
    "}\n",
    "\n",
    "for fname, end_sec in fabio_override.items():\n",
    "    working_df.loc[\n",
    "        working_df['filename'] == fname,\n",
    "        'valid_end_sec'\n",
    "    ] = end_sec\n",
    "\n",
    "\n",
    "# cropæˆ¦ç•¥ã¯åŸºæœ¬center\n",
    "working_df[\"crop_strategy\"] = \"center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_zoneã‚’æº–å‚™ï¼ˆã™ã§ã«\"checked\"ã®ã¿ã€é‡è¤‡é™¤å»æ¸ˆã¿ï¼‰\n",
    "safe_zone = pd.read_csv(\"../data/processed/safe_zones/safe_zone_0428.csv\")\n",
    "safe_zone = safe_zone[safe_zone[\"check\"] == \"checked\"]\n",
    "safe_zone = safe_zone.drop_duplicates(subset=[\"filename\"])\n",
    "\n",
    "# start, endã‚’floatã«å¤‰æ›\n",
    "safe_zone[\"start\"] = pd.to_numeric(safe_zone[\"start\"], errors=\"coerce\")\n",
    "safe_zone[\"end\"] = pd.to_numeric(safe_zone[\"end\"], errors=\"coerce\")\n",
    "\n",
    "# safe_zoneã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘æŒã£ã¦ãã‚‹\n",
    "safe_zone_update = safe_zone[[\"filename\", \"start\", \"end\"]]\n",
    "\n",
    "# working_dfã‚‚ç”¨æ„ã•ã‚Œã¦ã„ã‚‹æƒ³å®š\n",
    "\n",
    "# working_dfã«safe_zoneã®start, endã‚’ãƒãƒ¼ã‚¸ã™ã‚‹\n",
    "working_df = working_df.merge(safe_zone_update, on=\"filename\", how=\"left\")\n",
    "\n",
    "# start, endãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ã«ã¤ã„ã¦ã€valid_start_sec, valid_end_secã‚’æ›¸ãæ›ãˆ\n",
    "working_df[\"valid_start_sec\"] = working_df[\"start\"].combine_first(working_df[\"valid_start_sec\"])\n",
    "working_df[\"valid_end_sec\"] = working_df[\"end\"].combine_first(working_df[\"valid_end_sec\"])\n",
    "\n",
    "# ä½¿ã„çµ‚ã‚ã£ãŸstart, endã‚«ãƒ©ãƒ ã‚’æ¶ˆã™ï¼ˆå¿…è¦ãªã‚‰ï¼‰\n",
    "working_df = working_df.drop(columns=[\"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Marked 0 samples as invalid (over 1000 per label)\n"
     ]
    }
   ],
   "source": [
    "working_df[\"is_valid_audio\"] = True\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã”ã¨ã«å‡¦ç†\n",
    "for label, group in working_df.groupby('primary_label'):\n",
    "    if len(group) > config.num_prune_samples:\n",
    "        # ãƒ©ãƒ³ãƒ€ãƒ ã«500ä»¶ã‚’æ®‹ã™ï¼ˆæ®‹ã‚Šã‚’is_valid_audio=Falseã«ï¼‰\n",
    "        keep_indices = group.sample(n=config.num_prune_samples, random_state=config.seed).index\n",
    "        drop_indices = group.index.difference(keep_indices)\n",
    "        working_df.loc[drop_indices, 'is_valid_audio'] = False\n",
    "        print(label)\n",
    "\n",
    "print(f\"âœ… Marked {(~working_df['is_valid_audio']).sum()} samples as invalid (over {config.num_prune_samples} per label)\")\n",
    "\n",
    "\n",
    "# # # === 2. é™¤å»å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ False ã«ä¸Šæ›¸ã ===\n",
    "# fabio_remove_filenames = [\n",
    "#     '1139490/CSA36385.ogg',\n",
    "#     '1462737/CSA36369.ogg',\n",
    "#     '1462737/CSA36380.ogg',\n",
    "#     '1462737/CSA36381.ogg',\n",
    "#     '1462737/CSA36386.ogg',\n",
    "#     '1462737/CSA36391.ogg',\n",
    "#     '1462737/CSA36395.ogg',\n",
    "#     '963335/CSA36374.ogg',\n",
    "#     '963335/CSA36375.ogg',\n",
    "# ]\n",
    "# working_df.loc[working_df[\"filename\"].isin(fabio_remove_filenames), \"is_valid_audio\"] = False\n",
    "\n",
    "# ãƒã‚¤ã‚ºé™¤å»ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š\n",
    "working_df[\"apply_denoise\"] = False\n",
    "\n",
    "# fabio_denoise_filenames = [\n",
    "#     '1462711/CSA36371.ogg',\n",
    "#     '1462711/CSA36379.ogg',\n",
    "#     '963335/CSA36372.ogg',\n",
    "#     '963335/CSA36377.ogg',\n",
    "# ]\n",
    "\n",
    "# working_df.loc[working_df[\"filename\"].isin(fabio_denoise_filenames), \"apply_denoise\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of augmentations: 0\n"
     ]
    }
   ],
   "source": [
    "# augmentationã®ãŸã‚ã®å‡¦ç†ï¼å„éŸ³æºã§ã©ã‚Œãã‚‰ã„å¢—ã‚„ã™ã®ã‹ã‚’äº‹å‰ã«æ±ºå®š\n",
    "\n",
    "# åˆæœŸåŒ–\n",
    "working_df['n_augment'] = 0\n",
    "working_df['multi_crop'] = False\n",
    "\n",
    "target_samples = int(config.TARGET_DURATION * config.FS)\n",
    "\n",
    "# valid_end_sec ãŒ None ãªã‚‰ duration_sec ã«è£œå®Œ\n",
    "working_df['valid_end_sec'] = working_df.apply(\n",
    "    lambda row: row['duration_sec'] if pd.isna(row['valid_end_sec']) else row['valid_end_sec'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# valid_start_sec ãŒ None ãªã‚‰ 0 ã«è£œå®Œï¼ˆå¿µã®ãŸã‚ï¼‰\n",
    "working_df['valid_start_sec'] = working_df['valid_start_sec'].fillna(0)\n",
    "\n",
    "# rareãªãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡º\n",
    "label_counts = working_df['primary_label'].value_counts().rename_axis(\"label\").reset_index(name=\"sample_count\")\n",
    "rare_labels = label_counts[label_counts['sample_count'] < config.num_rare_samples]['label'].tolist()\n",
    "\n",
    "# âœ… rareç¨®ã”ã¨ã« crop æ•°ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "for rare_label in rare_labels:\n",
    "    base_rows = working_df[working_df['primary_label'] == rare_label]\n",
    "    n_exist = len(base_rows)\n",
    "    n_needed = config.num_rare_samples - n_exist\n",
    "    n_aug_per_sample = math.ceil(n_needed / n_exist)\n",
    "\n",
    "    for idx, row in base_rows.iterrows():\n",
    "        usable_duration_sec = row['valid_end_sec'] - row['valid_start_sec']\n",
    "        usable_samples = int(usable_duration_sec * config.FS)\n",
    "\n",
    "        # å°‘ãªãã¨ã‚‚2å€ã«ã™ã‚‹\n",
    "        max_possible = usable_samples // target_samples\n",
    "        n_actual = min(n_aug_per_sample, max_possible)\n",
    "\n",
    "        if n_actual > 0:\n",
    "            working_df.at[idx, 'multi_crop'] = True\n",
    "            working_df.at[idx, 'n_augment'] = n_actual\n",
    "            \n",
    "            \n",
    "if not config.is_crop_aug:\n",
    "    working_df['n_augment'] = 0\n",
    "    working_df['multi_crop'] = False\n",
    "\n",
    "# num_augmented\n",
    "print(f\"Total number of augmentations: {working_df['n_augment'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_strategyã«åŸºã¥ã„ã¦éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå‡ºã™\n",
    "# ç¾çŠ¶centerã—ã‹ä½¿ã£ã¦ãªã„ã®ã§ã‚ã¾ã‚Šæ„å‘³ãŒãªã„ã‚³ãƒ¼ãƒ‰ï¼\n",
    "def crop_audio(audio_data: np.ndarray, target_samples: int, strategy='center'):\n",
    "    total_samples = len(audio_data)\n",
    "\n",
    "    if total_samples < target_samples:\n",
    "        n_copy = math.ceil(target_samples / total_samples)\n",
    "        audio_data = np.concatenate([audio_data] * n_copy)\n",
    "        total_samples = len(audio_data)\n",
    "\n",
    "    if strategy == 'head':\n",
    "        # 1ç§’é…ã‚‰ã›ã¦é–‹å§‹ï¼ˆãŸã ã—åã¾ã‚‰ãªã„å ´åˆã¯0ã‹ã‚‰ï¼‰\n",
    "        buffer = int(1.0 * config.FS)\n",
    "        start_idx = min(buffer, total_samples - target_samples)\n",
    "    elif strategy == 'tail':\n",
    "        start_idx = total_samples - target_samples\n",
    "    elif strategy == 'center':\n",
    "        start_idx = total_samples // 2 - target_samples // 2\n",
    "    elif strategy == 'random':\n",
    "        max_start = total_samples - target_samples\n",
    "        start_idx = np.random.randint(0, max_start + 1)\n",
    "    elif isinstance(strategy, (float, int)):\n",
    "        start_idx = int(strategy * config.FS)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    start_idx = max(0, min(start_idx, total_samples - target_samples))\n",
    "    end_idx = start_idx + target_samples\n",
    "    return audio_data[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:17:07.823753Z",
     "iopub.status.busy": "2025-03-17T13:17:07.823361Z",
     "iopub.status.idle": "2025-03-17T13:17:07.829972Z",
     "shell.execute_reply": "2025-03-17T13:17:07.828954Z",
     "shell.execute_reply.started": "2025-03-17T13:17:07.823724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# audioã‚’melã«å¤‰æ›\n",
    "def audio2melspec(audio_data):\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=config.FS,\n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH,\n",
    "        n_mels=config.N_MELS,\n",
    "        fmin=config.FMIN,\n",
    "        fmax=config.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éŸ³æºã‚’melã«å¤‰ãˆã‚‹å‡¦ç†ï¼ä¸¦åˆ—åŒ–ã«å¯¾å¿œ\n",
    "def process_row(row):\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(row.filepath, sr=config.FS)\n",
    "        target_samples = int(config.TARGET_DURATION * config.FS)\n",
    "\n",
    "        mel_list = []\n",
    "        name_list = []\n",
    "\n",
    "        # === æœ‰åŠ¹ç¯„å›²ã‚’ç§’ â†’ ã‚µãƒ³ãƒ—ãƒ«ã«å¤‰æ› ===\n",
    "        valid_start_sec = row.get(\"valid_start_sec\", 0)\n",
    "        valid_end_sec = row.get(\"valid_end_sec\", None)\n",
    "        duration_sec = len(audio_data) / config.FS\n",
    "\n",
    "        if pd.isna(valid_end_sec) or valid_end_sec is None:\n",
    "            valid_end_sec = duration_sec\n",
    "\n",
    "        valid_start_sample = int(valid_start_sec * config.FS)\n",
    "        valid_end_sample = int(valid_end_sec * config.FS)\n",
    "\n",
    "        usable_audio = audio_data[valid_start_sample:valid_end_sample]\n",
    "        total_usable_samples = len(usable_audio)\n",
    "\n",
    "        # === ã‚ªãƒªã‚¸ãƒŠãƒ« clip ===\n",
    "        strategy = row.crop_strategy\n",
    "        try:\n",
    "            strategy = float(strategy)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        clip = crop_audio(usable_audio, target_samples, strategy=\"center\")  # strategyã¯centerå›ºå®š or ä»»æ„ã§ã‚‚å¯\n",
    "        if len(clip) < target_samples:\n",
    "            clip = np.pad(clip, (0, target_samples - len(clip)), mode='constant')\n",
    "        mel = audio2melspec(clip)\n",
    "        if mel.shape != config.TARGET_SHAPE:\n",
    "            mel = cv2.resize(mel, config.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        mel_list.append(mel.astype(np.float32))\n",
    "        name_list.append(row.samplename)\n",
    "\n",
    "        # === n_augment ã«å¿œã˜ã¦ crop ===\n",
    "        n_aug = int(row.get(\"n_augment\", 0))\n",
    "        if n_aug <= 0:\n",
    "            return list(zip(name_list, mel_list)), None\n",
    "\n",
    "        interval = max((total_usable_samples - target_samples) // (n_aug + 1), 1)\n",
    "\n",
    "        for i in range(n_aug):\n",
    "            start_idx = min(i * interval, total_usable_samples - target_samples)\n",
    "            clip = usable_audio[start_idx: start_idx + target_samples]\n",
    "            if len(clip) < target_samples:\n",
    "                clip = np.pad(clip, (0, target_samples - len(clip)), mode='constant')\n",
    "            mel_crop = audio2melspec(clip)\n",
    "            if mel_crop.shape != config.TARGET_SHAPE:\n",
    "                mel_crop = cv2.resize(mel_crop, config.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "            mel_list.append(mel_crop.astype(np.float32))\n",
    "            name_list.append(f\"{row.samplename}_crop{i}\")\n",
    "\n",
    "        return list(zip(name_list, mel_list)), None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, (row.filepath, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 3\n",
      "Errors:\n",
      "  ../data/raw//train_audio/65448/iNat273090.ogg: division by zero\n",
      "  ../data/raw//train_audio/amekes/iNat522503.ogg: division by zero\n",
      "  ../data/raw//train_audio/amekes/iNat522505.ogg: division by zero\n"
     ]
    }
   ],
   "source": [
    "# melå¤‰æ›ã‚’ä¸¦åˆ—åŒ–\n",
    "results = Parallel(n_jobs=config.N_JOBS)(\n",
    "    delayed(process_row)(row) for _, row in working_df.iloc[:total_samples].iterrows()\n",
    ")\n",
    "\n",
    "# çµæœã®æ•´ç†\n",
    "all_bird_data = {}\n",
    "errors = []\n",
    "\n",
    "for result, err in results:\n",
    "    if result is not None:\n",
    "        for name, mel in result:\n",
    "            all_bird_data[name] = mel\n",
    "    if err is not None:\n",
    "        errors.append(err)\n",
    "        \n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "if errors:\n",
    "    print(\"Errors:\")\n",
    "    for filepath, error in errors:\n",
    "        print(f\"  {filepath}: {error}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… working_df_augmented created with 0 augmented rows.\n"
     ]
    }
   ],
   "source": [
    "# working_dfã«augmentã—ãŸãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’è¿½åŠ \n",
    "augmented_rows = []\n",
    "\n",
    "for _, row in working_df.iterrows():\n",
    "    n_aug = int(row.get('n_augment', 0))\n",
    "    if n_aug > 0:\n",
    "        for i in range(n_aug):\n",
    "            new_row = row.copy()\n",
    "            new_row['samplename'] = f\"{row.samplename}_crop{i}\"\n",
    "            augmented_rows.append(new_row)\n",
    "\n",
    "# DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "augmented_rows = pd.DataFrame(augmented_rows)\n",
    "working_df_augmented = pd.concat([working_df, augmented_rows], ignore_index=True)\n",
    "print(f\"âœ… working_df_augmented created with {len(augmented_rows)} augmented rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # äº‹å‰ã«foldã‚’æ±ºã‚ã¦ãŠãï¼5foldï¼\n",
    "\n",
    "\n",
    "# working_df_augmented['group_id'] = working_df_augmented['samplename'].map(lambda x: x.split('_crop')[0])\n",
    "\n",
    "# # fold åˆ—ã‚’åˆæœŸåŒ–\n",
    "# working_df_augmented['fold'] = -1\n",
    "\n",
    "# # âœ… stratify + group ä¸¡ç«‹ï¼\n",
    "# sgkf = StratifiedGroupKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)\n",
    "# groups = working_df_augmented['group_id']\n",
    "# labels = working_df_augmented['primary_label']\n",
    "\n",
    "# for fold_id, (_, val_idx) in enumerate(sgkf.split(working_df_augmented, labels, groups=groups)):\n",
    "#     working_df_augmented.loc[val_idx, 'fold'] = fold_id\n",
    "\n",
    "# fold ã‚’å›ºå®šã™ã‚‹\n",
    "train_0419 = pd.read_csv(\"../data/processed/mel_cleaned_0419/train.csv\")\n",
    "# cropå‰ã® fold æƒ…å ±ã‚’è¾æ›¸åŒ–\n",
    "fold_map = train_0419.set_index(\"samplename\")[\"fold\"].to_dict()\n",
    "\n",
    "# cropå¾Œã® working_df_augmented ã« fold ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "working_df_augmented[\"group_id\"] = working_df_augmented[\"samplename\"].map(lambda x: x.split(\"_crop\")[0])\n",
    "working_df_augmented[\"fold\"] = working_df_augmented[\"group_id\"].map(fold_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‰ Removed 0 rows marked as invalid audio.\n",
      "âœ… Final training set size: 28564\n"
     ]
    }
   ],
   "source": [
    "# === ç„¡åŠ¹ãªéŸ³æºã‚’é™¤å¤–ï¼ˆfoldä»˜ä¸å¾Œï¼‰ ===\n",
    "working_df_filtered = working_df_augmented[working_df_augmented[\"is_valid_audio\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"ğŸ“‰ Removed {len(working_df_augmented) - len(working_df_filtered)} rows marked as invalid audio.\")\n",
    "print(f\"âœ… Final training set size: {len(working_df_filtered)}\")\n",
    "\n",
    "working_df_augmented = working_df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Mel-spectrograms saved to: ../data/processed/melspec_20250428_1706/birdclef2025_melspec_5sec_256_256.npy\n",
      "ğŸ“¦ File size: 7142.73 MB\n",
      "ğŸ“ Example shape: (256, 256)\n",
      "ğŸ“ Config saved to: ../data/processed/melspec_20250428_1706/config.csv\n",
      "ğŸ“ Augmented training metadata saved to: ../data/processed/melspec_20250428_1706/train.csv\n",
      "ğŸ“Š Total rows: 28564\n"
     ]
    }
   ],
   "source": [
    "# melã¨working_dfã‚’ä¿å­˜ï¼working_dfã¯train.csvã¨ã—ã¦ä¿å­˜\n",
    "\n",
    "# 4mins\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# === JSTæ™‚åˆ»ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ ===\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "now = datetime.now(jst)\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# âœ… ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ debug ã«å¿œã˜ã¦åˆ†å²\n",
    "if config.debug:\n",
    "    output_dir = os.path.join(config.PROCESSED_DIR, \"data_debugs\")\n",
    "else:\n",
    "    output_dir = os.path.join(config.PROCESSED_DIR, f\"melspec_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 1. melã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã®ä¿å­˜ ===\n",
    "output_path = os.path.join(output_dir, \"birdclef2025_melspec_5sec_256_256.npy\")\n",
    "wrapped_array = np.array(all_bird_data, dtype=object)\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(wrapped_array, f, protocol=5)\n",
    "\n",
    "print(f\"\\nâœ… Mel-spectrograms saved to: {output_path}\")\n",
    "print(f\"ğŸ“¦ File size: {os.path.getsize(output_path) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"ğŸ“ Example shape: {next(iter(all_bird_data.values())).shape}\")\n",
    "\n",
    "# === 2. configã®ä¿å­˜ ===\n",
    "config_path = os.path.join(output_dir, \"config.csv\")\n",
    "config_dict = {k: v for k, v in vars(config).items() if not k.startswith(\"__\")}\n",
    "\n",
    "with open(config_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"key\", \"value\"])\n",
    "    for key, value in config_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"ğŸ“ Config saved to: {config_path}\")\n",
    "\n",
    "\n",
    "# âœ… train.csv ã¨ã—ã¦ä¿å­˜\n",
    "train_csv_path = os.path.join(output_dir, \"train.csv\")\n",
    "working_df_augmented.to_csv(train_csv_path, index=False)\n",
    "\n",
    "print(f\"ğŸ“ Augmented training metadata saved to: {train_csv_path}\")\n",
    "print(f\"ğŸ“Š Total rows: {len(working_df_augmented)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Foldä¸€è‡´: 28564 / 28564 (100.00%)\n",
      "ğŸ‰ Fold assignment is consistent across all records.\n"
     ]
    }
   ],
   "source": [
    "# train_0419: samplename â†’ fold ã®ãƒãƒƒãƒ—ã‚’ä½œæˆ\n",
    "fold_map = train_0419.set_index(\"samplename\")[\"fold\"].to_dict()\n",
    "\n",
    "# working_df_augmented å´ã®å…ƒ clip ã«å¯¾ã™ã‚‹fold\n",
    "working_df_augmented[\"group_id\"] = working_df_augmented[\"samplename\"].map(lambda x: x.split(\"_crop\")[0])\n",
    "working_df_augmented[\"fold_from_train0419\"] = working_df_augmented[\"group_id\"].map(fold_map)\n",
    "\n",
    "# æ¯”è¼ƒï¼šfoldåˆ—ã¨fold_from_train0419åˆ—ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹\n",
    "match = (working_df_augmented[\"fold\"] == working_df_augmented[\"fold_from_train0419\"])\n",
    "num_total = len(working_df_augmented)\n",
    "num_match = match.sum()\n",
    "\n",
    "print(f\"âœ… Foldä¸€è‡´: {num_match} / {num_total} ({100*num_match/num_total:.2f}%)\")\n",
    "\n",
    "# 100%ä¸€è‡´ã—ã¦ã„ãªã„ã¨ãã®å·®åˆ†ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
    "if num_match != num_total:\n",
    "    mismatches = working_df_augmented[~match][[\"samplename\", \"group_id\", \"fold\", \"fold_from_train0419\"]]\n",
    "    print(\"âŒ Fold mismatch detected!\")\n",
    "    display(mismatches.head())\n",
    "else:\n",
    "    print(\"ğŸ‰ Fold assignment is consistent across all records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fabioã®è§£èª¬ã€€å¿…ãšã—ã‚‚7secã§ã¯ãªã„\n",
    "# 48124/CSA36346.ogg 24secä»¥é™\n",
    "# 52884/CSA36344.ogg 55secä»¥é™\n",
    "# 52884/CSA36342.ogg 14secä»¥é™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>filename</th>\n",
       "      <th>author</th>\n",
       "      <th>valid_end_sec</th>\n",
       "      <th>duration_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>963335</td>\n",
       "      <td>963335/CSA36372.ogg</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>8.0</td>\n",
       "      <td>108.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>963335</td>\n",
       "      <td>963335/CSA36374.ogg</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>8.0</td>\n",
       "      <td>103.571344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>963335</td>\n",
       "      <td>963335/CSA36375.ogg</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>8.0</td>\n",
       "      <td>106.700375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>963335</td>\n",
       "      <td>963335/CSA36377.ogg</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>7.0</td>\n",
       "      <td>107.488969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>963335</td>\n",
       "      <td>963335/CSA36393.ogg</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>7.0</td>\n",
       "      <td>97.343281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    primary_label             filename             author  valid_end_sec  \\\n",
       "911        963335  963335/CSA36372.ogg  Fabio A. Sarria-S            8.0   \n",
       "912        963335  963335/CSA36374.ogg  Fabio A. Sarria-S            8.0   \n",
       "913        963335  963335/CSA36375.ogg  Fabio A. Sarria-S            8.0   \n",
       "914        963335  963335/CSA36377.ogg  Fabio A. Sarria-S            7.0   \n",
       "915        963335  963335/CSA36393.ogg  Fabio A. Sarria-S            7.0   \n",
       "\n",
       "     duration_sec  \n",
       "911    108.242500  \n",
       "912    103.571344  \n",
       "913    106.700375  \n",
       "914    107.488969  \n",
       "915     97.343281  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 19\n",
    "idx_list = working_df[working_df[\"collection\"] == \"CSA\"][\"primary_label\"].unique()\n",
    "df = working_df[working_df[\"primary_label\"] == f\"{idx_list[i]}\"][[\"primary_label\", \"filename\", \"author\", \"valid_end_sec\", \"duration_sec\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSAãƒã‚§ãƒƒã‚¯\n",
    "# æœ€å¾Œã«ã‚¹ãƒšã‚¤ãƒ³èªãŒå«ã¾ã‚Œã¦ã„ãªã„label\n",
    "# 1564122, 50186/CSA28885.ogg, 523060\n",
    "# 52884/CSA14875.ogg\n",
    "# 548639\n",
    "# 714022\n",
    "# 868458\n",
    "\n",
    "\n",
    "\n",
    "# ã‚¹ãƒšã‚¤ãƒ³èªæœ€åˆã«å«ã¾ã‚Œã‚‹ 4secãã‚‰ã„\n",
    "# 50186/CSA28885.ogg\n",
    "# 52884/CSA14875.ogg\n",
    "\n",
    "\n",
    "\n",
    "# è©±ã—ã¦ã‚‹äºº\n",
    "# Eliana Barona- CortÃ©s\n",
    "# Alexandra Butrago-Cardona\n",
    "# Fabio A. Sarria-S\n",
    "\n",
    "# äººã®å£°ã ã‘ã®ç®‡æ‰€\n",
    "# 24292/CSA34649.ogg 2min48ç§»è¡Œ\n",
    "# 24292/CSA34651.ogg 1min34ç§»è¡Œ\n",
    "# 476537/CSA35459.ogg 2min14ç§»è¡Œ\n",
    "# 476537/CSA35461.ogg 4min19ç§»è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primary_label\n",
       "grekis     990\n",
       "compau     808\n",
       "trokin     787\n",
       "roahaw     709\n",
       "banana     610\n",
       "          ... \n",
       "64862        2\n",
       "81930        2\n",
       "41778        2\n",
       "42087        2\n",
       "1139490      2\n",
       "Name: count, Length: 206, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ£’ã‚°ãƒ©ãƒ•\n",
    "\n",
    "working_df_augmented[\"primary_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_valid_ranges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m7\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# äººã®å£°ãŒãªã„ç¯„å›²ã‚’å–å¾—\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m valid_range_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_valid_ranges\u001b[49m(df)\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m valid_range_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_end_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_start_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_valid_ranges' is not defined"
     ]
    }
   ],
   "source": [
    "humanvoice_df = pd.read_csv(\"../data/processed/human_voice/human_voice_0425.csv\")\n",
    "df = working_df[working_df[\"filename\"].isin(humanvoice_df[\"filename\"])]\n",
    "df = df[df[\"collection\"] != \"CSA\"]\n",
    "# ç§’æ•°ãŒé•·ã„ã‚‚ã®ã«ã¯äººã®å£°ãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§å¯¾è±¡ã¨ã™ã‚‹\n",
    "df = df[df[\"duration_sec\"] > 7]\n",
    "\n",
    "# äººã®å£°ãŒãªã„ç¯„å›²ã‚’å–å¾—\n",
    "valid_range_df = extract_valid_ranges(df)\n",
    "df = valid_range_df.copy()\n",
    "df = df[(df[\"valid_end_sec\"] - df[\"valid_start_sec\"] <= 5)]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanvoice_df = pd.read_csv(\"../data/processed/human_voice/human_voice_0425.csv\")\n",
    "df = working_df[working_df[\"filename\"].isin(humanvoice_df[\"filename\"])]\n",
    "df = df[df[\"collection\"] != \"CSA\"]\n",
    "# ç§’æ•°ãŒé•·ã„ã‚‚ã®ã«ã¯äººã®å£°ãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§å¯¾è±¡ã¨ã™ã‚‹\n",
    "df = df[df[\"duration_sec\"] > 7]\n",
    "\n",
    "# äººã®å£°ãŒãªã„ç¯„å›²ã‚’å–å¾—\n",
    "valid_range_df = extract_valid_ranges(df)\n",
    "df = valid_range_df.copy()\n",
    "df = df[(df[\"valid_end_sec\"] - df[\"valid_start_sec\"] >= 3) & (df[\"valid_end_sec\"] - df[\"valid_start_sec\"] <= 5)]\n",
    "print(len(df))\n",
    "utils_lib.plot_and_play_audio(\"tropar/iNat1231453.ogg\", config.train_datadir)\n",
    "\n",
    "\n",
    "# 7sã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼é™¤å»ã—ãŸå¾Œã®éŸ³å£°ã®é•·ã•ãŒ3sä»¥ä¸Š5sä»¥ä¸‹ã®ã‚‚ã®ã‚’è©•ä¾¡ï¼26åˆ†ã‹ã‹ã£ãŸï¼å…¨124ä»¶\n",
    "# æ›¸ã„ã¦ãªã„éŸ³æºã¯å…¨durationãŒä½¿ãˆã‚‹ã¨ã„ã†æ„å‘³\n",
    "\n",
    "# 21211/XC882654.ogg 3.7sä»¥é™\n",
    "#  22976/iNat1192434.ogg 6sä»¥é™\n",
    "# 517119/iNat1282698.ogg äººã®å£°ï¼ä¿ç•™\n",
    "#  67252/iNat1257838.ogg å…¨ãåŒã˜éŸ³æºã‚ã‚Šï¼æ¶ˆã™\n",
    "# amekes/iNat863228.ogg äººã®å£°ï¼4sä»¥é™\n",
    "# amekes/iNat863229.ogg äººã®å£°ï¼ä¿ç•™\n",
    "# banana/iNat483177.ogg äººã®å£°ï¼ä¿ç•™\n",
    "# bkmtou1/iNat1073839.ogg äººã®å£°ï¼ä¿ç•™ï¼1.5ï½“ä»˜è¿‘ã«ãƒã‚¤ã‚ºã‚ã‚Š\n",
    "# bkmtou1/iNat959239.ogg 4sä»¥é™\n",
    "# blkvul/XC324818.ogg äººã®å£°ï¼ä¿ç•™\n",
    "# cargra1/iNat969137.ogg å…¨éƒ¨äººã®å£°ï¼æ¶ˆã™ Valentina\n",
    "# chbant1/iNat922630.ogg éŸ³ãŒã‹ã™ã‹ï¼äººã®å£°ï¼ä¿ç•™\n",
    "# colcha1/iNat583631.ogg å…¨ãåŒã˜éŸ³æºã‚ã‚Šï¼æ¶ˆã™ \n",
    "# compau/iNat346502.ogg äººã®å£°ï¼éŸ³æ¥½ï¼ä¿ç•™\n",
    "# crcwoo1/XC9296.ogg 2.5sä»¥å‰ï¼\n",
    "# greibi1/iNat624338.ogg äººã®å£°ï¼ä¿ç•™\n",
    "# grekis/iNat388305.ogg 8sä»¥é™\n",
    "#  grekis/iNat979187.ogg 9sã‹ã‚‰12ï½“\n",
    "#  grepot1/XC416484.ogg 4sä»¥å‰\n",
    "# rufmot1/XC403364.ogg ãƒã‚¤ã‚ºãŒã™ã”ã„\n",
    "# trokin/iNat1240220.ogg ã‚´ã‚½ã‚´ã‚½ï¼ä¿ç•™\n",
    "\n",
    "\n",
    "# 7sã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼é™¤å»ã—ãŸå¾Œã®éŸ³å£°ã®é•·ã•ãŒ3sä»¥ä¸‹ã®ã‚‚ã®ã‚’è©•ä¾¡ï¼å…¨72ä»¶. 20åˆ†ã‹ã‹ã£ãŸ\n",
    "# æ›¸ã„ã¦ãªã„éŸ³æºã¯å…¨durationãŒä½¿ãˆã‚‹ã¨ã„ã†æ„å‘³\n",
    "\n",
    "# é™¤å»ã™ã¹ãå ´æ‰€ã‚’æ›¸ã„ã¦ã„ãï¼\n",
    "#  21211/XC882650.ogg 9sä»¥é™\n",
    "# 65448/iNat273090.ogg 9sä»¥å‰\n",
    "# amekes/iNat522505.ogg äººã®æ­Œã„å£°ï¼æ¶ˆã—ã¦ã„ã„ï¼\n",
    "# banana/XC437586.ogg 4sã‹ã‚‰6.5s\n",
    "#  banana/XC524992.ogg 10ä»¥é™\n",
    "# blchaw1/iNat629789.ogg 15sä»¥å‰\n",
    "# grekis/iNat972136.ogg äººã®å£°ã‚ã‚Šï¼æ¶ˆã—ã¦ã„ã„\n",
    "# grekis/iNat998004.ogg 2sä»¥å‰\n",
    "#  laufal1/iNat19124.ogg 12sä»¥é™\n",
    "# laufal1/iNat428603.ogg äººã®å£°ã‚ã‚Šï¼ä¿ç•™\n",
    "#  laufal1/iNat469782.ogg äººã®å£°ã‚ã‚Šï¼ä¿ç•™\n",
    "# pirfly1/XC309684.ogg 3sã‹ã‚‰9s\n",
    "\n",
    "# rosspo1/iNat307857.ogg  å…¨éƒ¨äººã®å£°ï¼æ¶ˆã™\n",
    "# saffin/iNat307815.ogg å…¨éƒ¨äººã®å£°ï¼æ¶ˆã™ï¼author:  Abraham CA\n",
    "# trokin/XC879492.ogg ã‹ãªã‚Šäººã®å£°ã‚ã‚Šï¼ä¿ç•™\n",
    "# trokin/iNat537466.ogg trokin/XC879492.ogg ã¨åŒã˜éŸ³å£°ï¼æ¶ˆã™\n",
    "#  tropar/iNat1231453.ogg 2sã‹ã‚‰4s\n",
    "#  watjac1/XC43803.ogg äººã®å£°ï¼ä¿ç•™\n",
    "#  wbwwre1/iNat1224010.ogg äººã®å£°ï¼ä¿ç•™\n",
    "#  wbwwre1/iNat1224011.ogg wbwwre1/iNat1224010.oggã¨åŒã˜ï¼æ¶ˆã™ï¼\n",
    "\n",
    "\n",
    "# é™¤å»ãƒŸã‚¹\n",
    "# 555142/iNat761666.ogg OK\n",
    "# 65448/iNat273090.ogg ã»ã¼äººã®å£°ï¼9sä»¥é™OK\n",
    "# 65448/iNat313963.ogg OK\n",
    "# 65962/iNat1109807.ogg OK\n",
    "# 65962/iNat382337.ogg OK\n",
    "# 67252/XC952184.ogg  OK\n",
    "# 67252/iNat1257838.ogg "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
