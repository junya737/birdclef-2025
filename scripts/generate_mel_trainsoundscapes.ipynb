{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class CFG:\n",
    "    def __init__(self, mode=\"train\", kaggle_notebook=False, debug=False):\n",
    "        assert mode in [\"train\", \"inference\"], \"mode must be 'train' or 'inference'\"\n",
    "        self.mode = mode\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "            # kaggle notebookならここを変更\n",
    "            self.model_path = \"/kaggle/input/birdclef-2025-baseline-fold0-0404\"\n",
    "            \n",
    "            self.device = \"cpu\"\n",
    "            self.batch_size = 8\n",
    "            self.n_jobs = 3\n",
    "            \n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            self.train_csv = '../data/raw/train.csv'\n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes_small/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.spectrogram_npy = '../data/processed/mel-spec_0329/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            self.MODELS_DIR = \"../models/\"\n",
    "            \n",
    "            # ローカルならここを変更\n",
    "            self.model_path =  \"../models/fld0_sfzn1000_hd_hl512_zscr_vino/\"\n",
    "            \n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.batch_size = 32\n",
    "            self.n_jobs = 3\n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = 'efficientnet_b0'\n",
    "        self.pretrained = True if mode == \"train\" else False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.WINDOW_SIZE = 5\n",
    "        self.TARGET_DURATION = 5\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        self.N_FFT = 1024\n",
    "        self.HOP_LENGTH = 512\n",
    "        self.N_MELS = 148\n",
    "        self.FMIN = 20\n",
    "        self.FMAX = 16000\n",
    "        \n",
    "        self.seed = 42\n",
    "        \n",
    "        self.norm_method = \"minmax\"\n",
    "        \n",
    "        # smoothingの係数\n",
    "        self.smooth_center_weight = 0.6\n",
    "        self.smooth_neighbor_weight = 0.2\n",
    "        \n",
    "        # ===== Inference Mode =====\n",
    "        if mode == \"inference\":\n",
    "            self.use_tta = False\n",
    "            self.tta_count = 3\n",
    "            self.threshold = 0.5\n",
    "\n",
    "            self.use_specific_folds = False\n",
    "            self.folds = [0, 1, 2, 3, 4]  # Used only if use_specific_folds is True\n",
    "\n",
    "            self.debug_count = 3\n",
    "            self.ensemble_strategy = \"mean\" # \"mean\", \"max\", \"min\", \"median\" など\n",
    "            \n",
    "            \n",
    "            \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CFG(mode='inference', kaggle_notebook=False)\n",
    "\n",
    "if cfg.KAGGLE_NOTEBOOK:\n",
    "    !pip install -U openvino-telemetry  --no-index --find-links /kaggle/input/pip-hub\n",
    "    !pip install -U openvino  --no-index --find-links /kaggle/input/pip-hub\n",
    "    sys.path.append(\"/kaggle/input/birdclef-2025-libs/\")\n",
    "    \n",
    "from openvino.runtime import Core\n",
    "from module import models_lib, utils_lib, preprocess_lib, inference_lib\n",
    "\n",
    "# Set seed\n",
    "utils_lib.set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(mel_spec: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    mean = np.mean(mel_spec)\n",
    "    std = np.std(mel_spec)\n",
    "    return (mel_spec - mean) / (std + eps)\n",
    "\n",
    "\n",
    "def minmax_normalize(mel_spec: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    mel_min = np.min(mel_spec)\n",
    "    mel_max = np.max(mel_spec)\n",
    "    return (mel_spec - mel_min) / (mel_max - mel_min + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    if cfg.norm_method == \"zscore\":\n",
    "        mel_spec_norm = zscore_normalize(mel_spec_db)\n",
    "    elif cfg.norm_method == \"minmax\":\n",
    "        mel_spec_norm = minmax_normalize(mel_spec_db)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization method: {cfg.norm_method}\")\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel変換\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"1ファイル分のmelspecデータを返す（row_id, melspecのリスト）\"\"\"\n",
    "    dataset = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = int(segment_idx * cfg.FS * cfg.WINDOW_SIZE)\n",
    "            end = int(start + cfg.FS * cfg.WINDOW_SIZE)\n",
    "            segment_audio = audio_data[start:end]\n",
    "\n",
    "            mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "            row_id = f\"{soundscape_id}_{(segment_idx + 1) * cfg.WINDOW_SIZE}\"\n",
    "\n",
    "            dataset.append({\n",
    "                \"row_id\": row_id,\n",
    "                \"mel_spec\": mel_spec\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 並列化してmelspecを生成\n",
    "def generate_melspec_dataset(cfg):\n",
    "    test_dir = Path(cfg.test_soundscapes)\n",
    "    if not test_dir.exists():\n",
    "        print(f\"Test directory {test_dir} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    test_files = list(test_dir.glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        print(\"No test audio files found.\")\n",
    "        return []\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    results = Parallel(n_jobs=cfg.n_jobs)(\n",
    "        delayed(process_audio_file)(path, cfg) for path in tqdm(test_files, desc=\"Parallel melspec gen\")\n",
    "    )\n",
    "    dataset = [item for sublist in results for item in sublist]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98c7081daca4f5e8af7acc58de08444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel melspec gen:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating dataset...\")\n",
    "dataset = generate_melspec_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Full dataset saved to: ../data/processed/melspec_20250523_2321/birdclef2025_melspec_dataset_raw.npy\n",
      "📦 File size: 12.01 MB\n",
      "📊 Total samples: 48\n",
      "📐 Example row_id: H02_20230502_080500_5\n",
      "📐 Example mel_spec shape: (256, 256)\n",
      "📝 Config saved to: ../data/processed/melspec_20250523_2321/config.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# === JST時刻でディレクトリ作成 ===\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "now = datetime.now(jst)\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# ✅ 保存先フォルダを debug に応じて分岐\n",
    "if cfg.debug:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, \"data_debugs\")\n",
    "else:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, f\"melspec_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 保存用オブジェクトに変換して保存 ===\n",
    "wrapped_array = np.array(dataset, dtype=object)\n",
    "output_path = os.path.join(output_dir, \"mel_train_soundscapes.npy\")\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(wrapped_array, f, protocol=5)\n",
    "\n",
    "print(f\"\\n✅ Full dataset saved to: {output_path}\")\n",
    "print(f\"📦 File size: {os.path.getsize(output_path) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"📊 Total samples: {len(wrapped_array)}\")\n",
    "print(f\"📐 Example row_id: {wrapped_array[0]['row_id']}\")\n",
    "print(f\"📐 Example mel_spec shape: {wrapped_array[0]['mel_spec'].shape}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# === configをCSV形式で保存 ===\n",
    "config_path = os.path.join(output_dir, \"config.csv\")\n",
    "\n",
    "# __ で始まる特殊メンバは除外\n",
    "config_dict = {k: v for k, v in vars(cfg).items() if not k.startswith(\"__\")}\n",
    "\n",
    "# 保存\n",
    "with open(config_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"key\", \"value\"])\n",
    "    for key, value in config_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"📝 Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
