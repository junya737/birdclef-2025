{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class CFG:\n",
    "    def __init__(self, mode=\"train\", kaggle_notebook=False, debug=False):\n",
    "        assert mode in [\"train\", \"inference\"], \"mode must be 'train' or 'inference'\"\n",
    "        self.mode = mode\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "            # kaggle notebookãªã‚‰ã“ã“ã‚’å¤‰æ›´\n",
    "            self.model_path = \"/kaggle/input/birdclef-2025-baseline-fold0-0404\"\n",
    "            \n",
    "            self.device = \"cpu\"\n",
    "            self.batch_size = 8\n",
    "            self.n_jobs = 3\n",
    "            \n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            self.train_csv = '../data/raw/train.csv'\n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes_small/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.spectrogram_npy = '../data/processed/mel-spec_0329/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            self.MODELS_DIR = \"../models/\"\n",
    "            \n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«ãªã‚‰ã“ã“ã‚’å¤‰æ›´\n",
    "            self.model_path =  \"../models/fld0_sfzn1000_hd_hl512_zscr_vino/\"\n",
    "            \n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            self.batch_size = 32\n",
    "            self.n_jobs = 3\n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = 'efficientnet_b0'\n",
    "        self.pretrained = True if mode == \"train\" else False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.WINDOW_SIZE = 5\n",
    "        self.TARGET_DURATION = 5\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        self.N_FFT = 1024\n",
    "        self.HOP_LENGTH = 512\n",
    "        self.N_MELS = 148\n",
    "        self.FMIN = 20\n",
    "        self.FMAX = 16000\n",
    "        \n",
    "        self.seed = 42\n",
    "        \n",
    "        self.norm_method = \"minmax\"\n",
    "        \n",
    "        # smoothingã®ä¿‚æ•°\n",
    "        self.smooth_center_weight = 0.6\n",
    "        self.smooth_neighbor_weight = 0.2\n",
    "        \n",
    "        # ===== Inference Mode =====\n",
    "        if mode == \"inference\":\n",
    "            self.use_tta = False\n",
    "            self.tta_count = 3\n",
    "            self.threshold = 0.5\n",
    "\n",
    "            self.use_specific_folds = False\n",
    "            self.folds = [0, 1, 2, 3, 4]  # Used only if use_specific_folds is True\n",
    "\n",
    "            self.debug_count = 3\n",
    "            self.ensemble_strategy = \"mean\" # \"mean\", \"max\", \"min\", \"median\" ãªã©\n",
    "            \n",
    "            \n",
    "            \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CFG(mode='inference', kaggle_notebook=False)\n",
    "\n",
    "if cfg.KAGGLE_NOTEBOOK:\n",
    "    !pip install -U openvino-telemetry  --no-index --find-links /kaggle/input/pip-hub\n",
    "    !pip install -U openvino  --no-index --find-links /kaggle/input/pip-hub\n",
    "    sys.path.append(\"/kaggle/input/birdclef-2025-libs/\")\n",
    "    \n",
    "from openvino.runtime import Core\n",
    "from module import models_lib, utils_lib, preprocess_lib, inference_lib\n",
    "\n",
    "# Set seed\n",
    "utils_lib.set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(mel_spec: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    mean = np.mean(mel_spec)\n",
    "    std = np.std(mel_spec)\n",
    "    return (mel_spec - mean) / (std + eps)\n",
    "\n",
    "\n",
    "def minmax_normalize(mel_spec: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    mel_min = np.min(mel_spec)\n",
    "    mel_max = np.max(mel_spec)\n",
    "    return (mel_spec - mel_min) / (mel_max - mel_min + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    if cfg.norm_method == \"zscore\":\n",
    "        mel_spec_norm = zscore_normalize(mel_spec_db)\n",
    "    elif cfg.norm_method == \"minmax\":\n",
    "        mel_spec_norm = minmax_normalize(mel_spec_db)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization method: {cfg.norm_method}\")\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melå¤‰æ›\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"1ãƒ•ã‚¡ã‚¤ãƒ«åˆ†ã®melspecãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆrow_id, melspecã®ãƒªã‚¹ãƒˆï¼‰\"\"\"\n",
    "    dataset = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = int(segment_idx * cfg.FS * cfg.WINDOW_SIZE)\n",
    "            end = int(start + cfg.FS * cfg.WINDOW_SIZE)\n",
    "            segment_audio = audio_data[start:end]\n",
    "\n",
    "            mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "            row_id = f\"{soundscape_id}_{(segment_idx + 1) * cfg.WINDOW_SIZE}\"\n",
    "\n",
    "            dataset.append({\n",
    "                \"row_id\": row_id,\n",
    "                \"mel_spec\": mel_spec\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ä¸¦åˆ—åŒ–ã—ã¦melspecã‚’ç”Ÿæˆ\n",
    "def generate_melspec_dataset(cfg):\n",
    "    test_dir = Path(cfg.test_soundscapes)\n",
    "    if not test_dir.exists():\n",
    "        print(f\"Test directory {test_dir} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    test_files = list(test_dir.glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        print(\"No test audio files found.\")\n",
    "        return []\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    results = Parallel(n_jobs=cfg.n_jobs)(\n",
    "        delayed(process_audio_file)(path, cfg) for path in tqdm(test_files, desc=\"Parallel melspec gen\")\n",
    "    )\n",
    "    dataset = [item for sublist in results for item in sublist]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98c7081daca4f5e8af7acc58de08444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel melspec gen:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating dataset...\")\n",
    "dataset = generate_melspec_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Full dataset saved to: ../data/processed/melspec_20250523_2321/birdclef2025_melspec_dataset_raw.npy\n",
      "ðŸ“¦ File size: 12.01 MB\n",
      "ðŸ“Š Total samples: 48\n",
      "ðŸ“ Example row_id: H02_20230502_080500_5\n",
      "ðŸ“ Example mel_spec shape: (256, 256)\n",
      "ðŸ“ Config saved to: ../data/processed/melspec_20250523_2321/config.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# === JSTæ™‚åˆ»ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ ===\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "now = datetime.now(jst)\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# âœ… ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’ debug ã«å¿œã˜ã¦åˆ†å²\n",
    "if cfg.debug:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, \"data_debugs\")\n",
    "else:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, f\"melspec_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === ä¿å­˜ç”¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã¦ä¿å­˜ ===\n",
    "wrapped_array = np.array(dataset, dtype=object)\n",
    "output_path = os.path.join(output_dir, \"mel_train_soundscapes.npy\")\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(wrapped_array, f, protocol=5)\n",
    "\n",
    "print(f\"\\nâœ… Full dataset saved to: {output_path}\")\n",
    "print(f\"ðŸ“¦ File size: {os.path.getsize(output_path) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"ðŸ“Š Total samples: {len(wrapped_array)}\")\n",
    "print(f\"ðŸ“ Example row_id: {wrapped_array[0]['row_id']}\")\n",
    "print(f\"ðŸ“ Example mel_spec shape: {wrapped_array[0]['mel_spec'].shape}\")\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# === configã‚’CSVå½¢å¼ã§ä¿å­˜ ===\n",
    "config_path = os.path.join(output_dir, \"config.csv\")\n",
    "\n",
    "# __ ã§å§‹ã¾ã‚‹ç‰¹æ®Šãƒ¡ãƒ³ãƒã¯é™¤å¤–\n",
    "config_dict = {k: v for k, v in vars(cfg).items() if not k.startswith(\"__\")}\n",
    "\n",
    "# ä¿å­˜\n",
    "with open(config_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"key\", \"value\"])\n",
    "    for key, value in config_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"ðŸ“ Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
