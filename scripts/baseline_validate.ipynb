{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'module.config_lib' from '/root/program/birdclef-2025/scripts/module/config_lib.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import timm\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from module import preprocess_lib, datasets_lib, utils_lib, models_lib, learning_lib, config_lib\n",
    "reload(config_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(targets, probs):\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "        aucs = [roc_auc_score(targets_bin[:, i], probs[:, i])\n",
    "                for i in range(targets.shape[1]) if np.sum(targets_bin[:, i]) > 0]\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "def calculate_map(targets, probs):\n",
    "    targets_bin = (targets >= 0.5).astype(int)\n",
    "    aps = [average_precision_score(targets_bin[:, i], probs[:, i])\n",
    "            for i in range(targets.shape[1]) if np.sum(targets_bin[:, i]) > 0]\n",
    "    return np.mean(aps) if aps else 0.0\n",
    "\n",
    "class CFG:\n",
    "    def __init__(self, mode=\"train\", kaggle_notebook=False, debug=False):\n",
    "        assert mode in [\"train\", \"inference\"], \"mode must be 'train' or 'inference'\"\n",
    "        self.mode = mode\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            self.model_path = '/kaggle/input/birdclef-2025-0330'\n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.RAW_DIR = '../data/raw/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            self.train_csv = '../data/raw/train.csv'\n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.models_dir = \"../models/\" # ÂÖ®model„ÅÆ‰øùÂ≠òÂÖà\n",
    "            self.model_path = self.models_dir # ÂêÑ„É¢„Éá„É´„ÅÆ‰øùÂ≠òÂÖàÔºéÂ≠¶ÁøíÊôÇ„Å´ÂãïÁöÑ„Å´Â§âÊõ¥Ôºé\n",
    "            \n",
    "            self.spectrogram_npy = '../data/processed/baseline/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "            self.pseudo_label_csv = \"../data/result/pseudo_labels_baseline_7sec.csv\"\n",
    "            self.pseudo_melspec_npy = \"../data/processed/train_soundscapes_0407/train_soundscapes_melspecs.npy\"\n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = 'efficientnet_b0'\n",
    "        self.pretrained = True if mode == \"train\" else False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.WINDOW_SIZE = 5.0 # Êé®Ë´ñÊôÇ„ÅÆ„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫\n",
    "        self.TARGET_DURATION = 5.0 # „Éá„Éº„Çø„Çª„ÉÉ„Éà‰ΩúÊàêÊôÇ„ÅÆ„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        self.N_FFT = 1024\n",
    "        self.HOP_LENGTH = 512\n",
    "        self.N_MELS = 128\n",
    "        self.FMIN = 50\n",
    "        self.FMAX = 14000        \n",
    "\n",
    "        # ===== Training Mode =====\n",
    "        if mode == \"train\":\n",
    "            self.seed = 42\n",
    "            self.apex = False\n",
    "            self.print_freq = 100\n",
    "            self.num_workers = 2\n",
    "\n",
    "            self.LOAD_DATA = True\n",
    "            self.epochs = 10\n",
    "            self.batch_size = 32\n",
    "            self.criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "            self.n_fold = 5\n",
    "            self.selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "            self.optimizer = 'AdamW'\n",
    "            self.lr = 5e-4\n",
    "            self.weight_decay = 1e-5\n",
    "            self.scheduler = 'CosineAnnealingLR'\n",
    "            self.min_lr = 1e-6\n",
    "            self.T_max = self.epochs\n",
    "\n",
    "            self.aug_prob = 0.5\n",
    "            self.mixup_alpha_real = 0.5\n",
    "            self.mixup_alpha_pseudo = 0.5\n",
    "            \n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            self.use_pseudo_mixup = False  # pseudo lable„Åßmixup„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            self.pseudo_mix_prob = 0.4  # mixup„Åßpseudo lable„Çí‰Ωø„ÅÜÁ¢∫Áéá\n",
    "            self.pseudo_conf_threshold = 0.5\n",
    "            \n",
    "\n",
    "            if self.debug:\n",
    "                self.epochs = 2\n",
    "                self.selected_folds = [0]\n",
    "                self.batch_size = 4\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug true„Å´„Åô„Çã„Å®validation„ÅÆÊï∞„Åå1000„Å´Âõ∫ÂÆö„Åï„Çå„ÇãÔºé\n",
    "cfg = CFG(mode=\"train\", kaggle_notebook=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_lib.set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BirdCLEFValidator:\n",
    "    def __init__(self, cfg, df, datasets_lib, models_lib):\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.datasets_lib = datasets_lib\n",
    "        self.models_lib = models_lib\n",
    "        self.label2index = {}\n",
    "        self.index2label = {}\n",
    "        self.spectrograms = None\n",
    "        self.val_metrics = {}\n",
    "\n",
    "        self._load_taxonomy()\n",
    "        self._load_spectrograms()\n",
    "\n",
    "    def _load_taxonomy(self):\n",
    "        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        species_ids = taxonomy_df['primary_label'].tolist()\n",
    "        self.cfg.num_classes = len(species_ids)\n",
    "        self.index2label = {i: label for i, label in enumerate(species_ids)}\n",
    "        self.label2index = {label: i for i, label in enumerate(species_ids)}\n",
    "\n",
    "    def _load_spectrograms(self):\n",
    "        print(f\"Loading pre-computed mel spectrograms from: {self.cfg.spectrogram_npy}\")\n",
    "        self.spectrograms = np.load(self.cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "        print(f\"Loaded {len(self.spectrograms)} spectrograms\")\n",
    "\n",
    "    def _get_val_df(self, fold):\n",
    "        skf = StratifiedKFold(n_splits=self.cfg.n_fold, shuffle=True, random_state=self.cfg.seed)\n",
    "        _, val_idx = list(skf.split(self.df, self.df['primary_label']))[fold]\n",
    "        return self.df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    def _get_val_loader(self, val_df):\n",
    "        val_dataset = self.datasets_lib.BirdCLEFDatasetFromNPY(val_df, self.cfg, self.spectrograms, mode='valid')\n",
    "        return DataLoader(val_dataset, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "                          num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "                          collate_fn=self.datasets_lib.collate_fn)\n",
    "\n",
    "    def _load_model(self, model_path):\n",
    "        model = self.models_lib.BirdCLEFModelForTrain(self.cfg).to(self.cfg.device)\n",
    "        state = torch.load(model_path, map_location=self.cfg.device)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model.eval()\n",
    "    \n",
    "    def _calculate_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # üëá ROC AUC „ÅØ„Éê„Ç§„Éä„É™„É©„Éô„É´„ÇíÂøÖË¶Å„Å®„Åô„Çã„ÅÆ„Åß„ÄÅsoft label„Çí2ÂÄ§Âåñ\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        aucs = [roc_auc_score(targets_bin[:, i], probs[:, i]) \n",
    "                for i in range(targets.shape[1]) if np.sum(targets_bin[:, i]) > 0]\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    def _calculate_classwise_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # „Éê„Ç§„Éä„É™ÂåñÔºàÈÄ£Á∂öÂÄ§„Åß„ÇÇint„Åß„ÇÇÂÆâÂÖ®Ôºâ\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_auc = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_auc[i] = roc_auc_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_auc[i] = np.nan  # „Ç®„É©„ÉºÂá∫„Åü„Å®„Åç„ÇÇÂÆâÂøÉ\n",
    "        return classwise_auc\n",
    "\n",
    "    def _calculate_classwise_ap(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # „É©„Éô„É´„Çí„Éê„Ç§„Éä„É™ÂåñÔºàsoft labelÂØæÂøúÔºâ\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_ap = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_ap[i] = average_precision_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_ap[i] = np.nan\n",
    "        return classwise_ap\n",
    "    \n",
    "    def _calculate_map(self, targets, outputs):\n",
    "        classwise_ap = self._calculate_classwise_ap(targets, outputs)\n",
    "        values = [v for v in classwise_ap.values() if v is not None and not np.isnan(v)]\n",
    "        return np.mean(values) if values else 0.0\n",
    "\n",
    "    def _predict(self, model, loader):\n",
    "        model.eval()\n",
    "        all_outputs, all_targets, all_filenames = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Validation\"):\n",
    "                if isinstance(batch['melspec'], list):\n",
    "                    for melspec, target, filename in zip(batch['melspec'], batch['target'], batch['filename']):\n",
    "                        inputs = melspec.unsqueeze(0).to(self.cfg.device)\n",
    "                        output = model(inputs)\n",
    "                        output = output[0] if isinstance(output, tuple) else output\n",
    "                        all_outputs.append(output.detach().cpu().numpy())\n",
    "                        all_targets.append(target.numpy())\n",
    "                        all_filenames.append(filename)\n",
    "                else:\n",
    "                    inputs = batch['melspec'].to(self.cfg.device)\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                    outputs = outputs.detach().cpu().numpy()\n",
    "                    targets = batch['target'].numpy()\n",
    "                    all_outputs.extend(outputs)\n",
    "                    all_targets.extend(targets)\n",
    "                    all_filenames.extend(batch['filename'])\n",
    "        \n",
    "        all_outputs = np.array(all_outputs)\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_filenames = np.array(all_filenames)\n",
    "        \n",
    "        self.val_metrics = {\n",
    "            'val_auc': self._calculate_auc(all_targets, all_outputs),\n",
    "            \"val_map\": self._calculate_map(all_targets, all_outputs),\n",
    "            \"val_classwise_auc\": self._calculate_classwise_auc(all_targets, all_outputs),\n",
    "            \"val_classwise_ap\": self._calculate_classwise_ap(all_targets, all_outputs),\n",
    "        }\n",
    "\n",
    "        return all_outputs, all_targets, all_filenames\n",
    "    \n",
    "\n",
    "    def evaluate_model_dir(self, model_dir):\n",
    "        full_dir = os.path.join(self.cfg.models_dir, model_dir)\n",
    "        print(f\"\\nüîç Evaluating model directory: {full_dir}\")\n",
    "\n",
    "        for fold in range(self.cfg.n_fold):\n",
    "            model_path = os.path.join(full_dir, f\"model_fold{fold}.pth\")\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"‚õîÔ∏è model_fold{fold}.pth not found in {model_dir}\")\n",
    "                continue\n",
    "\n",
    "            val_df = self._get_val_df(fold)\n",
    "            val_loader = self._get_val_loader(val_df)\n",
    "            self.val_loader = val_loader\n",
    "\n",
    "            model = self._load_model(model_path)\n",
    "            outputs, targets, filenames = self._predict(model, val_loader)\n",
    "\n",
    "            class_names = [self.index2label[i] for i in range(outputs.shape[1])]\n",
    "            probs = 1 / (1 + np.exp(-outputs))\n",
    "            df_preds = pd.DataFrame(probs, columns=class_names)\n",
    "            df_preds.insert(0, \"row_id\", filenames)\n",
    "            df_preds.to_csv(os.path.join(full_dir, f\"predictions_fold{fold}.csv\"), index=False)\n",
    "            \n",
    "            df_targets = pd.DataFrame(targets, columns=class_names)\n",
    "            df_targets.insert(0, \"row_id\", filenames)\n",
    "            df_targets.to_csv(os.path.join(full_dir, f\"targets_fold{fold}.csv\"), index=False)\n",
    "            \n",
    "            print(\"Val AUC:\", f\"{self.val_metrics['val_auc']:.4f}\", \"Val MAP:\", f\"{self.val_metrics['val_map']:.4f}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Finished evaluation for model_dir: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n",
      "Loading pre-computed mel spectrograms from: ../data/processed/baseline/birdclef2025_melspec_5sec_256_256.npy\n",
      "Loaded 28564 spectrograms\n",
      "\n",
      "üîç Evaluating model directory: ../models/baseline_pseudo_th0.5\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0536ba98c03458a8a0ba6a91f7cb62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.9424 Val MAP: 0.5033\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8db678051a9487ebaf9e4f5e15227da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.9455 Val MAP: 0.5235\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cc9592515e4736b138814f52062b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.9484 Val MAP: 0.5218\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eb2624679749da847cdf755e6d7530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.9474 Val MAP: 0.5185\n",
      "Found 5712 matching spectrograms for valid dataset out of 5712 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39192fc38284e38a1f5055a4edbe8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.9469 Val MAP: 0.5294\n",
      "\n",
      "‚úÖ Finished evaluation for model_dir: baseline_pseudo_th0.5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "\n",
    "    model_dirs = [\n",
    "    \"baseline_pseudo_th0.5\",\n",
    "    ]\n",
    "    validator = BirdCLEFValidator(cfg, train_df, datasets_lib, models_lib)\n",
    "    # ====== ÂÆüË°å ======\n",
    "    for model_dir in model_dirs:\n",
    "        validator.evaluate_model_dir(model_dir)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 validation\n",
      "Ensemble AUC: 0.950\n",
      "Ensemble MAP: 0.529\n",
      "Average AUC: 0.950\n",
      "Average MAP: 0.529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def weighted_ensemble(prediction_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction_dict (dict): keys are model paths to CSV, values are weights (float)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: ensembled predictions (row_id + class probs)\n",
    "    \"\"\"\n",
    "    assert len(prediction_dict) > 0, \"No predictions provided.\"\n",
    "\n",
    "    dfs = []\n",
    "    weights = []\n",
    "\n",
    "    for path, weight in prediction_dict.items():\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "        weights.append(weight)\n",
    "\n",
    "    row_ids = dfs[0]['row_id']\n",
    "    class_names = dfs[0].columns[1:]\n",
    "\n",
    "    # stack predictions and apply weights\n",
    "    stacked = np.stack([df[class_names].values * w for df, w in zip(dfs, weights)], axis=0)\n",
    "    ensemble_preds = np.sum(stacked, axis=0) / np.sum(weights)\n",
    "\n",
    "    df_ensemble = pd.DataFrame(ensemble_preds, columns=class_names)\n",
    "    df_ensemble.insert(0, \"row_id\", row_ids)\n",
    "    return df_ensemble\n",
    "\n",
    "\n",
    "\n",
    "selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "all_aucs = []\n",
    "all_maps = []\n",
    "\n",
    "for fold in selected_folds:\n",
    "    print(f\"Fold {fold} validation\")\n",
    "    \n",
    "    targets = pd.read_csv(os.path.join(cfg.models_dir, \"baseline_7sec\", f\"targets_fold{fold}.csv\"))\n",
    "    model_1_path = os.path.join(cfg.models_dir, \"baseline_7sec\", f\"predictions_fold{fold}.csv\")\n",
    "    model_2_path = os.path.join(cfg.models_dir, \"baseline_pseudo_th0.5\", f\"predictions_fold{fold}.csv\")\n",
    "    pred_dict = {\n",
    "        model_1_path: 0.6,\n",
    "        model_2_path: 0.4,\n",
    "    }\n",
    "\n",
    "    df_ens = weighted_ensemble(pred_dict)\n",
    "    \n",
    "    auc = calculate_auc(targets.iloc[:, 1:].values, df_ens.iloc[:, 1:].values)\n",
    "    map = calculate_map(targets.iloc[:, 1:].values, df_ens.iloc[:, 1:].values)\n",
    "    \n",
    "    all_aucs.append(auc)\n",
    "    all_maps.append(map)\n",
    "    \n",
    "    print(\"Ensemble AUC:\", f\"{auc:.3f}\")\n",
    "    print(\"Ensemble MAP:\", f\"{map:.3f}\")\n",
    "    \n",
    "print(\"Average AUC:\", f\"{np.mean(all_aucs):.3f}\")\n",
    "print(\"Average MAP:\", f\"{np.mean(all_maps):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.9498 with weights: (0.6, 0.4)\n"
     ]
    }
   ],
   "source": [
    "def grid_search_weights(pred_paths, targets, resolution=11):\n",
    "    best_auc = -1\n",
    "    best_weights = None\n",
    "    best_df = None\n",
    "\n",
    "    weights_grid = [(w/10, 1 - w/10) for w in range(resolution)]\n",
    "\n",
    "    for w1, w2 in weights_grid:\n",
    "        pred_dict = {\n",
    "            pred_paths[0]: w1,\n",
    "            pred_paths[1]: w2,\n",
    "        }\n",
    "        df_ens = weighted_ensemble(pred_dict)\n",
    "        auc = calculate_auc(targets.iloc[:, 1:].values, df_ens.iloc[:, 1:].values)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_weights = (w1, w2)\n",
    "            best_df = df_ens\n",
    "\n",
    "    print(f\"Best AUC: {best_auc:.4f} with weights: {best_weights}\")\n",
    "    return best_df, best_weights\n",
    "\n",
    "df_ens, best_weights = grid_search_weights([model_1_path, model_2_path], targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val AUC: 0.942\n",
      "Val MAP: 0.466\n"
     ]
    }
   ],
   "source": [
    "# 7sec„ÅÆ„É¢„Éá„É´„ÅØÔºåÂ≠¶ÁøíÊôÇ„ÅÆ„Çπ„Ç≥„Ç¢„Å®‰∏ÄËá¥„Åó„Å™„ÅÑ„ÅÆ„ÅØ‰ªïÊßòÔºé7sec„ÅÆ„É¢„Éá„É´„ÅØdataset„ÇÇ7secÔºé„Åì„ÅÆNB„Åß„ÅØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ5s„Å™„ÅÆ„ÅßÊ≠£„Åó„ÅÑË©ï‰æ°\n",
    "model_dir_name = \"baseline_fold0_7sec\"\n",
    "model_path = os.path.join(cfg.models_dir, model_dir_name)\n",
    "predictions = pd.read_csv(os.path.join(model_path, \"predictions_fold0.csv\"))\n",
    "targets = pd.read_csv(os.path.join(model_path, \"targets_fold0.csv\"))\n",
    "log = pd.read_csv(os.path.join(model_path, \"log_fold0.csv\"))\n",
    "\n",
    "print(\"Val AUC:\", f\"{calculate_auc(targets.iloc[:, 1:].values, predictions.iloc[:, 1:].values):.3f}\")\n",
    "print(\"Val MAP:\", f\"{calculate_map(targets.iloc[:, 1:].values, predictions.iloc[:, 1:].values):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Val AUC: 0.942, Val MAP: 0.503\n",
      "Fold 1: Val AUC: 0.946, Val MAP: 0.523\n",
      "Fold 2: Val AUC: 0.948, Val MAP: 0.522\n",
      "Fold 3: Val AUC: 0.947, Val MAP: 0.519\n",
      "Fold 4: Val AUC: 0.947, Val MAP: 0.529\n",
      "Mean AUC: 0.946, Mean MAP: 0.519\n"
     ]
    }
   ],
   "source": [
    "# 7sec„ÅÆ„É¢„Éá„É´„ÅØÔºåÂ≠¶ÁøíÊôÇ„ÅÆ„Çπ„Ç≥„Ç¢„Å®‰∏ÄËá¥„Åó„Å™„ÅÑ„ÅÆ„ÅØ‰ªïÊßòÔºé7sec„ÅÆ„É¢„Éá„É´„ÅØdataset„ÇÇ7secÔºé„Åì„ÅÆNB„Åß„ÅØ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ5s„Å™„ÅÆ„ÅßÊ≠£„Åó„ÅÑË©ï‰æ°\n",
    "model_dir_name = \"baseline_pseudo_th0.5\"\n",
    "model_path = os.path.join(cfg.models_dir, model_dir_name)\n",
    "\n",
    "all_aucs = []\n",
    "all_maps = []\n",
    "\n",
    "for fold in range(5):\n",
    "    predictions = pd.read_csv(os.path.join(model_path, f\"predictions_fold{fold}.csv\"))\n",
    "    targets = pd.read_csv(os.path.join(model_path, f\"targets_fold{fold}.csv\"))\n",
    "    log = pd.read_csv(os.path.join(model_path, f\"log_fold{fold}.csv\"))\n",
    "    \n",
    "    auc = calculate_auc(targets.iloc[:, 1:].values, predictions.iloc[:, 1:].values)\n",
    "    map = calculate_map(targets.iloc[:, 1:].values, predictions.iloc[:, 1:].values)\n",
    "    all_aucs.append(auc)\n",
    "    all_maps.append(map)\n",
    "    print(f\"Fold {fold}: Val AUC: {auc:.3f}, Val MAP: {map:.3f}\")\n",
    "print(f\"Mean AUC: {np.mean(all_aucs):.3f}, Mean MAP: {np.mean(all_maps):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
