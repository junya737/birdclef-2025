{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    debug = True  \n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 2\n",
    "    KAGGLE_NOTEBOOK = False\n",
    "    \n",
    "    if KAGGLE_NOTEBOOK:\n",
    "        OUTPUT_DIR = '/kaggle/working/'\n",
    "        train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "        train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "        test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "        submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "        taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "        spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "    else:\n",
    "        OUTPUT_DIR = '../data/result/'\n",
    "        train_datadir = '../data/raw/train_audio/'\n",
    "        train_csv = '../data/raw/train.csv'\n",
    "        test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "        submission_csv = '../data/raw/sample_submission.csv'\n",
    "        taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "        spectrogram_npy = '../data/processed/mel-spec_0329/birdclef2025_melspec_5sec_256_256.npy'\n",
    "        MODEL_DIR = \"../models/\"\n",
    "\n",
    " \n",
    "    model_name = 'efficientnet_b0'  \n",
    "    pretrained = True\n",
    "    in_channels = 1\n",
    "\n",
    "    LOAD_DATA = True  \n",
    "    FS = 32000\n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    \n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 10  \n",
    "    batch_size = 32  \n",
    "    criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "    n_fold = 5\n",
    "    selected_folds = [0, 1, 2, 3, 4]   \n",
    "\n",
    "    optimizer = 'AdamW'\n",
    "    lr = 5e-4 \n",
    "    weight_decay = 1e-5\n",
    "  \n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = epochs\n",
    "\n",
    "    aug_prob = 0.5  \n",
    "    mixup_alpha = 0.5  \n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [0]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, spectrograms=None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "\n",
    "        self.spectrograms = spectrograms\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        self.species_ids = taxonomy_df['primary_label'].tolist()\n",
    "        self.num_classes = len(self.species_ids)\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.species_ids)}\n",
    "\n",
    "        if 'filepath' not in self.df.columns:\n",
    "            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df.filename\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        sample_names = set(self.df['samplename'])\n",
    "        if self.spectrograms:\n",
    "            found_samples = sum(1 for name in sample_names if name in self.spectrograms)\n",
    "            print(f\"Found {found_samples} matching spectrograms for {mode} dataset out of {len(self.df)} samples\")\n",
    "        \n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        samplename = row['samplename']\n",
    "        spec = None\n",
    "\n",
    "        if self.spectrograms and samplename in self.spectrograms:\n",
    "            spec = self.spectrograms[samplename]\n",
    "        elif not self.cfg.LOAD_DATA:\n",
    "            spec = process_audio_file(row['filepath'], self.cfg)\n",
    "\n",
    "        if spec is None:\n",
    "            spec = np.zeros(self.cfg.TARGET_SHAPE, dtype=np.float32)\n",
    "            if self.mode == \"train\":  # Only print warning during training\n",
    "                print(f\"Warning: Spectrogram for {samplename} not found and could not be generated\")\n",
    "\n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and random.random() < self.cfg.aug_prob:\n",
    "            spec = self.apply_spec_augmentations(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'])\n",
    "        \n",
    "        if 'secondary_labels' in row and row['secondary_labels'] not in [[''], None, np.nan]:\n",
    "            if isinstance(row['secondary_labels'], str):\n",
    "                secondary_labels = eval(row['secondary_labels'])\n",
    "            else:\n",
    "                secondary_labels = row['secondary_labels']\n",
    "            \n",
    "            for label in secondary_labels:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename']\n",
    "        }\n",
    "    \n",
    "    def apply_spec_augmentations(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "    \n",
    "        # Time masking (horizontal stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                width = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[2] - width)\n",
    "                spec[0, :, start:start+width] = 0\n",
    "        \n",
    "        # Frequency masking (vertical stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                height = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[1] - height)\n",
    "                spec[0, start:start+height, :] = 0\n",
    "        \n",
    "        # Random brightness/contrast\n",
    "        if random.random() < 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            bias = random.uniform(-0.1, 0.1)\n",
    "            spec = spec * gain + bias\n",
    "            spec = torch.clamp(spec, 0, 1) \n",
    "            \n",
    "        return spec\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        \"\"\"Encode label to one-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label in self.label_to_idx:\n",
    "            target[self.label_to_idx[label]] = 1.0\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "        \n",
    "        self.mixup_enabled = hasattr(cfg, 'mixup_alpha') and cfg.mixup_alpha > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "    \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            mixed_x, targets_a, targets_b, lam = self.mixup_data(x, targets)\n",
    "            x = mixed_x\n",
    "        else:\n",
    "            targets_a, targets_b, lam = None, None, None\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits, \n",
    "                                       logits, targets_a, targets_b, lam)\n",
    "            return logits, loss\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    def mixup_data(self, x, targets):\n",
    "        \"\"\"Applies mixup to the data batch\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "\n",
    "        indices = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[indices]\n",
    "        \n",
    "        return mixed_x, targets, targets[indices], lam\n",
    "    \n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        \"\"\"Applies mixup to the loss function\"\"\"\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = None  \n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "   \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, cfg):\n",
    "    \"\"\"Training function that can either use pre-computed spectrograms or generate them on-the-fly\"\"\"\n",
    "\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    species_ids = taxonomy_df['primary_label'].tolist()\n",
    "    cfg.num_classes = len(species_ids)\n",
    "    \n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    spectrograms = None\n",
    "    if cfg.LOAD_DATA:\n",
    "        print(\"Loading pre-computed mel spectrograms from NPY file...\")\n",
    "        try:\n",
    "            spectrograms = np.load(cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "            print(f\"Loaded {len(spectrograms)} pre-computed mel spectrograms\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-computed spectrograms: {e}\")\n",
    "            print(\"Will generate spectrograms on-the-fly instead.\")\n",
    "            cfg.LOAD_DATA = False\n",
    "    \n",
    "    if not cfg.LOAD_DATA:\n",
    "        print(\"Will generate spectrograms on-the-fly during training.\")\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.train_datadir + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    \n",
    "    best_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "            \n",
    "        print(f'\\n{\"=\"*30} Fold {fold} {\"=\"*30}')\n",
    "        \n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f'Training set: {len(train_df)} samples')\n",
    "        print(f'Validation set: {len(val_df)} samples')\n",
    "        \n",
    "        train_dataset = BirdCLEFDatasetFromNPY(train_df, cfg, spectrograms=spectrograms, mode='train')\n",
    "        val_dataset = BirdCLEFDatasetFromNPY(val_df, cfg, spectrograms=spectrograms, mode='valid')\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        model = BirdCLEFModel(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        \n",
    "        if cfg.scheduler == 'OneCycleLR':\n",
    "            scheduler = lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=cfg.lr,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=cfg.epochs,\n",
    "                pct_start=0.1\n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_scheduler(optimizer, cfg)\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(cfg.epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "            \n",
    "            train_loss, train_auc = train_one_epoch(\n",
    "                model, \n",
    "                train_loader, \n",
    "                optimizer, \n",
    "                criterion, \n",
    "                cfg.device,\n",
    "                scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "            \n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "            \n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch + 1\n",
    "                print(f\"New best AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': val_auc,\n",
    "                    'train_auc': train_auc,\n",
    "                    'cfg': cfg\n",
    "                }, f\"{cfg.MODEL_DIR}/model_fold{fold}.pth\")\n",
    "        \n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n",
      "\n",
      "Starting training...\n",
      "LOAD_DATA is set to True\n",
      "Using pre-computed mel spectrograms from NPY file\n",
      "Loading pre-computed mel spectrograms from NPY file...\n",
      "Loaded 28564 pre-computed mel spectrograms\n",
      "\n",
      "============================== Fold 0 ==============================\n",
      "Training set: 22851 samples\n",
      "Validation set: 5713 samples\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafd693f1bff4d4fab84bd4fc807d7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab63152bdbc41378925f00e7bc7f88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1823, Train AUC: 0.4915\n",
      "Val Loss: 0.0477, Val AUC: 0.4879\n",
      "New best AUC: 0.4879 at epoch 1\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca67d6db5fa946adb68ddb062f583930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91b972bfa994fdabd45d8682f77da68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0320, Train AUC: 0.4863\n",
      "Val Loss: 0.0319, Val AUC: 0.5108\n",
      "New best AUC: 0.5108 at epoch 2\n",
      "\n",
      "Best AUC for fold 0: 0.5108 at epoch 2\n",
      "\n",
      "============================================================\n",
      "Cross-Validation Results:\n",
      "Fold 0: 0.5108\n",
      "Mean AUC: 0.5108\n",
      "============================================================\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"LOAD_DATA is set to {cfg.LOAD_DATA}\")\n",
    "    if cfg.LOAD_DATA:\n",
    "        print(\"Using pre-computed mel spectrograms from NPY file\")\n",
    "    else:\n",
    "        print(\"Will generate spectrograms on-the-fly during training\")\n",
    "    \n",
    "    run_training(train_df, cfg)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
