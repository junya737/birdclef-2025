{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'module.config_lib' from '/root/program/birdclef-2025/scripts/module/config_lib.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import timm\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from module import preprocess_lib, datasets_lib, utils_lib, models_lib, learning_lib, config_lib\n",
    "reload(config_lib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self, mode=\"train\", kaggle_notebook=False, debug=False):\n",
    "        assert mode in [\"train\", \"inference\"], \"mode must be 'train' or 'inference'\"\n",
    "        self.mode = mode\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            \n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.model_path = '/kaggle/input/birdclef-2025-0330' \n",
    "            \n",
    "            # kaggle notebookならここを変更する．\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.RAW_DIR = '../data/raw/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            \n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.models_dir = \"../models/\" # 全modelの保存先\n",
    "            self.model_path = self.models_dir # 各モデルの保存先．学習時に動的に変更．\n",
    "            self.pseudo_label_csv = \"../data/processed/pseudo_labels/ensemble_7sec_pseudoth0.5/pseudo_label.csv\"\n",
    "            self.pseudo_melspec_npy = \"../data/processed/train_soundscapes_0407/train_soundscapes_melspecs.npy\"\n",
    "\n",
    "            # ローカルならここを変更する．\n",
    "            self.train_csv = '../data/processed/pretrain_0422/train.csv'\n",
    "            self.spectrogram_npy = '../data/processed/pretrain_0422/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = 'tf_efficientnetv2_b3' # tf_efficientnetv2_b3\n",
    "        self.pretrained = True if mode == \"train\" else False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.WINDOW_SIZE = 5.0 # 推論時のウィンドウサイズ\n",
    "        self.TARGET_DURATION = 5.0 # データセット作成時のウィンドウサイズ\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        \n",
    "        # trainer内部で決まるのでここでは指定しない．\n",
    "        self.num_classes = None\n",
    "\n",
    "\n",
    "        # ===== Training Mode =====\n",
    "        if mode == \"train\":\n",
    "            self.seed = 42\n",
    "            self.apex = False\n",
    "            self.print_freq = 100\n",
    "            self.num_workers = 2\n",
    "\n",
    "            self.LOAD_DATA = True\n",
    "            self.epochs = 10\n",
    "            self.batch_size = 32\n",
    "            self.criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "            self.n_fold = 5\n",
    "            self.selected_folds = [0]\n",
    "\n",
    "            self.optimizer = 'AdamW'\n",
    "            self.lr = 5e-4\n",
    "            self.weight_decay = 1e-5\n",
    "            self.scheduler = 'CosineAnnealingLR'\n",
    "            self.min_lr = 1e-6\n",
    "            self.T_max = self.epochs\n",
    "            \n",
    "           \n",
    "            \n",
    "            ## 現状使ってない．\n",
    "            self.aug_prob = 0.5\n",
    "            self.mixup_alpha_real = 0.5\n",
    "            self.mixup_alpha_pseudo = 0.5\n",
    "            self.use_pseudo_mixup = False  # pseudo lableでmixupするかどうか\n",
    "            self.pseudo_mix_prob = 0.4  # mixupでpseudo lableを使う確率\n",
    "            self.pseudo_conf_threshold = 0.5\n",
    "            self.full_train = False\n",
    "            ###\n",
    "            \n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            \n",
    "            if self.debug:\n",
    "                self.epochs = 2\n",
    "                self.selected_folds = [0]\n",
    "                self.batch_size = 4\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CFG(mode=\"train\", kaggle_notebook=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_lib.set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainの処理をクラスで実行．\n",
    "class BirdCLEFTrainer:\n",
    "    def __init__(self, cfg, df, taxonomy_df, datasets_lib, models_lib, learning_lib):\n",
    "        self.cfg = cfg\n",
    "        self.df = df.head(100).reset_index(drop=True) if cfg.debug else df\n",
    "        self.taxonomy_df = taxonomy_df\n",
    "        self.datasets_lib = datasets_lib\n",
    "        self.models_lib = models_lib\n",
    "        self.learning_lib = learning_lib\n",
    "        self.spectrograms = None\n",
    "        self.pseudo_df = None\n",
    "        self.pseudo_melspecs = None\n",
    "        self.best_scores = []\n",
    "        self.train_metrics = {}\n",
    "        self.val_metrics = {}\n",
    "        self.label2index = {}\n",
    "        self.index2label = {}\n",
    "        self.num_classes = None\n",
    "\n",
    "        self._setup_model_dir()\n",
    "        self._save_config()\n",
    "        self._build_index_label_mapping()\n",
    "        self._load_spectrograms()\n",
    "        \n",
    "        if self.cfg.use_pseudo_mixup:\n",
    "            self._load_pseudo_data()\n",
    "\n",
    "    def _setup_model_dir(self):\n",
    "        if self.cfg.debug:\n",
    "            current_time = \"debug\"\n",
    "            self.cfg.model_path = os.path.join(self.cfg.models_dir, \"models_debug\")\n",
    "        else:\n",
    "            japan_time = datetime.now(timezone(timedelta(hours=9)))\n",
    "            current_time = japan_time.strftime('%Y%m%d_%H%M')\n",
    "            self.cfg.model_path = os.path.join(self.cfg.models_dir, f\"models_{current_time}\")\n",
    "\n",
    "        os.makedirs(self.cfg.model_path, exist_ok=True)\n",
    "        print(f\"[INFO] Models will be saved to: {self.cfg.model_path}\")\n",
    "\n",
    "        # dataset-metadata.jsonを保存\n",
    "        dataset_metadata = {\n",
    "            \"title\": f\"bc25-models-{current_time}\",\n",
    "            \"id\": f\"ihiratch/bc25-models-{current_time}\",\n",
    "            \"licenses\": [\n",
    "                {\n",
    "                    \"name\": \"CC0-1.0\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        metadata_path = os.path.join(self.cfg.model_path, \"dataset-metadata.json\")\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "    def _save_config(self):\n",
    "        cfg_dict = vars(self.cfg)\n",
    "        cfg_df = pd.DataFrame(list(cfg_dict.items()), columns=[\"key\", \"value\"])\n",
    "        cfg_df.to_csv(os.path.join(self.cfg.model_path, \"config.csv\"), index=False)\n",
    "\n",
    "    def _build_index_label_mapping(self):\n",
    "        species_ids = self.taxonomy_df['primary_label'].tolist()\n",
    "        self.cfg.num_classes = len(species_ids)\n",
    "        # labelとindexの対応\n",
    "        self.index2label = {i: label for i, label in enumerate(species_ids)}\n",
    "        self.label2index = {label: i for i, label in enumerate(species_ids)}\n",
    "\n",
    "        print(self.index2label)\n",
    "\n",
    "    def _load_spectrograms(self):\n",
    "        print(f\"Loading pre-computed mel spectrograms from NPY file, from the path: {self.cfg.spectrogram_npy}\")\n",
    "        self.spectrograms = np.load(self.cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "        print(f\"Loaded {len(self.spectrograms)} pre-computed mel spectrograms\")\n",
    "        \n",
    "    def _load_pseudo_data(self):\n",
    "        print(\"📥 Loading pseudo label CSV and melspecs...\")\n",
    "\n",
    "        # row_id を index にして読み込む（← ここがポイント！）\n",
    "        self.pseudo_df = pd.read_csv(self.cfg.pseudo_label_csv, index_col=\"row_id\")\n",
    "\n",
    "        # 信頼度フィルタリング（例: 最大値が 0.5 未満の行を除く）\n",
    "        confidence_threshold = self.cfg.pseudo_conf_threshold\n",
    "        max_probs = self.pseudo_df.max(axis=1)\n",
    "        self.pseudo_df = self.pseudo_df[max_probs > confidence_threshold]\n",
    "        self.pseudo_df = self.pseudo_df.reset_index(drop=False)\n",
    "        print(f\"✅ Filtered pseudo labels: {len(self.pseudo_df)}\")\n",
    "\n",
    "        # melspec は key が row_id の dict を想定\n",
    "        self.pseudo_melspecs = np.load(self.cfg.pseudo_melspec_npy, allow_pickle=True)\n",
    "        print(f\"✅ Loaded pseudo mel-spectrograms: {len(self.pseudo_melspecs)}\")\n",
    "        \n",
    "    def _create_train_dataset(self, train_df):\n",
    "        if self.cfg.use_pseudo_mixup:\n",
    "            print(\"🔀 Using BirdCLEFDatasetWithPseudo (with pseudo label mixup)\")\n",
    "            return self.datasets_lib.BirdCLEFDatasetWithPseudo(\n",
    "                train_df=train_df,\n",
    "                pseudo_df=self.pseudo_df,\n",
    "                cfg=self.cfg,\n",
    "                spectrograms=self.spectrograms,\n",
    "                pseudo_melspecs=self.pseudo_melspecs,\n",
    "                mode='train'\n",
    "            )\n",
    "        else:\n",
    "            print(\"📦 Using BirdCLEFDatasetFromNPY (no pseudo mixup)\")\n",
    "\n",
    "            return self.datasets_lib.BirdCLEFDatasetFromNPY_Labeled(\n",
    "                    df=train_df,\n",
    "                    cfg=self.cfg,\n",
    "                    spectrograms=self.spectrograms,\n",
    "                    mode=\"train\",\n",
    "                    label2idx=self.label2index,\n",
    "                    idx2label=self.index2label \n",
    "                    )\n",
    "            \n",
    "\n",
    "    def _calculate_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # 👇 ROC AUC はバイナリラベルを必要とするので、soft labelを2値化\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        aucs = [roc_auc_score(targets_bin[:, i], probs[:, i]) \n",
    "                for i in range(targets.shape[1]) if np.sum(targets_bin[:, i]) > 0]\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    def _calculate_classwise_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # バイナリ化（連続値でもintでも安全）\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_auc = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_auc[i] = roc_auc_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_auc[i] = np.nan  # エラー出たときも安心\n",
    "        return classwise_auc\n",
    "\n",
    "    def _calculate_classwise_ap(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # ラベルをバイナリ化（soft label対応）\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_ap = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_ap[i] = average_precision_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_ap[i] = np.nan\n",
    "        return classwise_ap\n",
    "    \n",
    "    def _calculate_map(self, targets, outputs):\n",
    "        classwise_ap = self._calculate_classwise_ap(targets, outputs)\n",
    "        values = [v for v in classwise_ap.values() if v is not None and not np.isnan(v)]\n",
    "        return np.mean(values) if values else 0.0\n",
    "\n",
    "    def _save_classwise_scores_to_csv(self, classwise_auc, classwise_ap, fold, filename_prefix):\n",
    "        rows = []\n",
    "        for i in classwise_auc:\n",
    "            label = self.index2label.get(i, str(i))\n",
    "            auc = classwise_auc[i]\n",
    "            ap = classwise_ap.get(i, np.nan)\n",
    "            rows.append({\"label\": label, \"val_auc\": auc, \"val_ap\": ap})\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(os.path.join(self.cfg.model_path, f\"{filename_prefix}_classwise_score_fold{fold}.csv\"), index=False)\n",
    "\n",
    "\n",
    "    def train_one_epoch(self, model, loader, optimizer, criterion, device, scheduler=None):\n",
    "        model.train()\n",
    "        losses, all_targets, all_outputs = [], [], []\n",
    "\n",
    "        pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "        for step, batch in pbar:\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs, batch_losses = [], []\n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                optimizer.step()\n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = outputs[1] if isinstance(outputs, tuple) else criterion(outputs, targets)\n",
    "                outputs = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            if scheduler and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss.item() if not isinstance(loss, float) else loss)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        self.train_metrics = {\n",
    "            'train_loss': np.mean(losses),\n",
    "            'train_auc': self._calculate_auc(all_targets, all_outputs),\n",
    "            \"train_map\": self._calculate_map(all_targets, all_outputs),   \n",
    "            \"train_classwise_auc\": self._calculate_classwise_auc(all_targets, all_outputs),\n",
    "            \"train_classwise_ap\": self._calculate_classwise_ap(all_targets, all_outputs),  \n",
    "        }\n",
    "\n",
    "    def validate(self, model, loader, criterion, device):\n",
    "        model.eval()\n",
    "        losses, all_targets, all_outputs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Validation\"):\n",
    "                if isinstance(batch['melspec'], list):\n",
    "                    batch_outputs, batch_losses = [], []\n",
    "                    for i in range(len(batch['melspec'])):\n",
    "                        inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                        target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                        output = model(inputs)\n",
    "                        loss = criterion(output, target)\n",
    "                        batch_outputs.append(output.detach().cpu())\n",
    "                        batch_losses.append(loss.item())\n",
    "                    outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                    loss = np.mean(batch_losses)\n",
    "                    targets = batch['target'].numpy()\n",
    "                else:\n",
    "                    inputs = batch['melspec'].to(device)\n",
    "                    targets = batch['target'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    outputs = outputs.detach().cpu().numpy()\n",
    "                    targets = targets.detach().cpu().numpy()\n",
    "\n",
    "                all_outputs.append(outputs)\n",
    "                all_targets.append(targets)\n",
    "                losses.append(loss.item() if not isinstance(loss, float) else loss)\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        # print(\"Size of validation:\",  len(all_targets))\n",
    "        self.val_metrics = {\n",
    "            'val_loss': np.mean(losses),\n",
    "            'val_auc': self._calculate_auc(all_targets, all_outputs),\n",
    "            \"val_map\": self._calculate_map(all_targets, all_outputs),\n",
    "            \"val_classwise_auc\": self._calculate_classwise_auc(all_targets, all_outputs),\n",
    "            \"val_classwise_ap\": self._calculate_classwise_ap(all_targets, all_outputs),\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        for fold in range(self.cfg.n_fold):\n",
    "            if fold not in self.cfg.selected_folds:\n",
    "                continue\n",
    "            print(f\"\\n{'='*30} Fold {fold} {'='*30}\")\n",
    "\n",
    "            # train.csvのfoldを使う．\n",
    "            \n",
    "            if self.cfg.full_train:\n",
    "                train_df = self.df.reset_index(drop=True)\n",
    "                val_df = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "                print(\"Use full train data for training.\")\n",
    "            else:\n",
    "                train_df = self.df[self.df['fold'] != fold].reset_index(drop=True)\n",
    "                val_df = self.df[self.df['fold'] == fold].reset_index(drop=True) \n",
    "            \n",
    "            print(f\"Training set: {len(train_df)} samples\")\n",
    "            print(f\"Validation set: {len(val_df)} samples\")\n",
    "\n",
    "            train_dataset = self._create_train_dataset(train_df)\n",
    "            val_dataset = self.datasets_lib.BirdCLEFDatasetFromNPY_Labeled(\n",
    "                        df=val_df,\n",
    "                        cfg=self.cfg,\n",
    "                        spectrograms=self.spectrograms,\n",
    "                        mode='valid',\n",
    "                        label2idx=self.label2index,\n",
    "                        idx2label=self.index2label\n",
    "                    )\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.cfg.batch_size, shuffle=True, \n",
    "                                       num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "                                       collate_fn=self.datasets_lib.collate_fn, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "                                     num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "                                     collate_fn=self.datasets_lib.collate_fn)\n",
    "\n",
    "            model = self.models_lib.BirdCLEFModelForTrain(self.cfg).to(self.cfg.device)\n",
    "            optimizer = self.learning_lib.get_optimizer(model, self.cfg)\n",
    "            criterion = self.learning_lib.get_criterion(self.cfg)\n",
    "\n",
    "            scheduler = (lr_scheduler.OneCycleLR(optimizer, max_lr=self.cfg.lr, \n",
    "                        steps_per_epoch=len(train_loader), epochs=self.cfg.epochs, pct_start=0.1)\n",
    "                         if self.cfg.scheduler == 'OneCycleLR'\n",
    "                         else self.learning_lib.get_scheduler(optimizer, self.cfg))\n",
    "\n",
    "            best_auc = 0\n",
    "            log_history = []\n",
    "\n",
    "            for epoch in range(self.cfg.epochs):\n",
    "                print(f\"\\nEpoch {epoch+1}/{self.cfg.epochs}\")\n",
    "                start_time = time.time()\n",
    "\n",
    "                self.train_one_epoch(model, train_loader, optimizer, criterion, self.cfg.device, scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None)\n",
    "                self.validate(model, val_loader, criterion, self.cfg.device)\n",
    "\n",
    "                # スコア取得\n",
    "                train_loss = self.train_metrics['train_loss']\n",
    "                train_auc = self.train_metrics['train_auc']\n",
    "                train_auc_map = self.train_metrics['train_map']\n",
    "\n",
    "                val_loss = self.val_metrics['val_loss']\n",
    "                val_auc = self.val_metrics['val_auc']\n",
    "                val_auc_map = self.val_metrics['val_map']\n",
    "                val_classwise_auc = self.val_metrics['val_classwise_auc']\n",
    "                val_classwise_ap = self.val_metrics['val_classwise_ap']\n",
    "\n",
    "                if scheduler and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step(val_loss if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau) else None)\n",
    "\n",
    "                print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}, Train MAP: {train_auc_map:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val MAP: {val_auc_map:.4f}\")\n",
    "\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    print(f\"New best AUC: {best_auc:.4f} at epoch {epoch+1}\")\n",
    "                    \n",
    "                    self._save_classwise_scores_to_csv(val_classwise_auc, val_classwise_ap, fold, filename_prefix=\"best_val\")\n",
    "\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'epoch': epoch,\n",
    "                        'val_auc': val_auc,\n",
    "                        'train_auc': train_auc,\n",
    "                        \"index2label\": self.index2label,\n",
    "                        'cfg': self.cfg\n",
    "                    }, f\"{self.cfg.model_path}/model_fold{fold}.pth\")\n",
    "\n",
    "                log_entry = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'lr': scheduler.get_last_lr()[0] if scheduler else self.cfg.lr,\n",
    "                    'epoch_time_min': round((time.time() - start_time) / 60, 2)\n",
    "                }\n",
    "\n",
    "                # classwiseスコアを除外した val_metrics のログ\n",
    "                train_log = {f\"{k}\": v for k, v in self.train_metrics.items() if not k.startswith(\"train_classwise\")}\n",
    "                val_log = {f\"{k}\": v for k, v in self.val_metrics.items() if not k.startswith(\"val_classwise\")}\n",
    "                \n",
    "                # ログ用スコアの更新（classwiseは除外）\n",
    "                log_entry.update(train_log)\n",
    "                log_entry.update(val_log)\n",
    "                log_history.append(log_entry)\n",
    "            \n",
    "           \n",
    "                \n",
    "\n",
    "            pd.DataFrame(log_history).to_csv(f\"{self.cfg.model_path}/log_fold{fold}.csv\", index=False)\n",
    "            self.best_scores.append(best_auc)\n",
    "            print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f}\")\n",
    "\n",
    "            del model, optimizer, scheduler, train_loader, val_loader\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Cross-Validation Results:\")\n",
    "        for fold, score in enumerate(self.best_scores):\n",
    "            print(f\"Fold {self.cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "        print(f\"Mean AUC: {np.mean(self.best_scores):.4f}\")\n",
    "        print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_865362/1795578204.py:4: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(cfg.train_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "[INFO] Models will be saved to: ../models/models_debug\n",
      "{0: 'comgre', 1: 'goflea1', 2: 'orihob2', 3: 'nilfly2', 4: 'comkin1', 5: 'houspa', 6: 'kenplo1', 7: 'brnhao1', 8: 'ingori1', 9: 'rossta2', 10: 'insbab1', 11: 'grefla1', 12: 'emedov2', 13: 'blakit1', 14: 'rocpig', 15: 'purher1', 16: 'sohmyn1', 17: 'chbeat1', 18: 'gargan', 19: 'bwfshr1', 20: 'comfla1', 21: 'barfly1', 22: 'asiope1', 23: 'moipig1', 24: 'grnwar1', 25: 'indrol2', 26: 'placuc3', 27: 'bkrfla1', 28: 'insowl1', 29: 'vehpar1', 30: 'eaywag1', 31: 'nutman', 32: 'graher1', 33: 'ashpri1', 34: 'yebbab1', 35: 'grehor1', 36: 'spodov', 37: 'rufwoo2', 38: 'hoopoe', 39: 'niwpig1', 40: 'comtai1', 41: 'brodro1', 42: 'whcbar1', 43: 'grejun2', 44: 'integr', 45: 'whbtre1', 46: 'pomgrp2', 47: 'grewar3', 48: 'junmyn1', 49: 'copbar1', 50: 'grecou1', 51: 'comros', 52: 'whrmun', 53: 'yebbul3', 54: 'isbduc1', 55: 'crfbar1', 56: 'indrob1', 57: 'smamin1', 58: 'maltro1', 59: 'grtdro1', 60: 'inpher1', 61: 'dafbab1', 62: 'grnsan', 63: 'whbsho3', 64: 'whbwag1', 65: 'comior1', 66: 'categr', 67: 'inbrob1', 68: 'whbbul2', 69: 'tilwar1', 70: 'bkwsti', 71: 'spepic1', 72: 'barswa', 73: 'litgre1', 74: 'rorpar', 75: 'bkskit1', 76: 'malwoo1', 77: 'blhori1', 78: 'cohcuc1', 79: 'wbbfly1', 80: 'blnmon1', 81: 'whbwoo2', 82: 'putbab1', 83: 'indpit1', 84: 'compea', 85: 'rewlap1', 86: 'whiter2', 87: 'brwjac1', 88: 'ruftre2', 89: 'scamin3', 90: 'bladro1', 91: 'blrwar1', 92: 'pursun4', 93: 'brwowl1', 94: 'forwag1', 95: 'wynlau1', 96: 'crseag1', 97: 'spoowl1', 98: 'paisto1', 99: 'litspi1', 100: 'pursun3', 101: 'litegr', 102: 'comsan', 103: 'commyn', 104: 'ashwoo2', 105: 'eurbla2', 106: 'gyhcaf1', 107: 'kerlau2', 108: 'grbeat1', 109: 'vefnut1', 110: 'btbeat1', 111: 'lblwar1', 112: 'whtkin2', 113: 'brasta1', 114: 'rerswa1', 115: 'piekin1', 116: 'bncwoo3', 117: 'maghor2', 118: 'wemhar1', 119: 'sttwoo1', 120: 'rewbul', 121: 'junbab2', 122: 'houcro1', 123: 'labcro1', 124: 'indtit1', 125: 'brfowl1', 126: 'aspfly1', 127: 'darter2', 128: 'zitcis1', 129: 'heswoo1', 130: 'rufbab3', 131: 'gybpri1', 132: 'malpar1', 133: 'gloibi', 134: 'sbeowl1', 135: 'lirplo', 136: 'shikra1', 137: 'revbul', 138: 'jerbus2', 139: 'redspu1', 140: 'asbfly', 141: 'brcful1', 142: 'purswa3', 143: 'lewduc1', 144: 'eucdov', 145: 'gryfra', 146: 'brnshr', 147: 'blaeag1', 148: 'aspswi1', 149: 'grenig1', 150: 'cregos1', 151: 'woosan', 152: 'mawthr1', 153: 'brakit1', 154: 'crbsun2', 155: 'lesyel1', 156: 'eurcoo', 157: 'piebus1', 158: 'tibfly3', 159: 'ashdro1', 160: 'lobsun2', 161: 'plhpar1', 162: 'oripip1', 163: 'grywag', 164: 'pabflo1', 165: 'grynig2', 166: 'stbkin1', 167: 'litswi1', 168: 'asikoe2', 169: 'marsan', 170: 'plapri1', 171: 'thbwar1', 172: 'whbwat1', 173: 'laudov1', 174: 'commoo3', 175: 'plaflo1', 176: 'bcnher', 177: 'sqtbul1', 178: 'junowl1', 179: 'bkcbul1', 180: 'rutfly6', 181: 'yewgre1', 182: 'abhori1', 183: 'abethr1', 184: 'abythr1', 185: 'afbfly1', 186: 'afdfly1', 187: 'afecuc1', 188: 'affeag1', 189: 'afghor1', 190: 'afgfly1', 191: 'afmdov1', 192: 'afpfly1', 193: 'afpwag1', 194: 'afpkin1', 195: 'afrgos1', 196: 'afrgrp1', 197: 'afrjac1', 198: 'afrthr1', 199: 'amesun2', 200: 'augbuz1', 201: 'bagwea1', 202: 'bawhor2', 203: 'bcbeat1', 204: 'bawman1', 205: 'beasun2', 206: 'bkctch1', 207: 'bkfruw1', 208: 'blacra1', 209: 'blacuc1', 210: 'blaplo1', 211: 'blbpuf2', 212: 'blcapa2', 213: 'blfbus1', 214: 'blhgon1', 215: 'blksaw1', 216: 'blhher1', 217: 'blnmou1', 218: 'blnwea1', 219: 'bltbar1', 220: 'bltori1', 221: 'bltapa1', 222: 'blwlap1', 223: 'brctch1', 224: 'brcale1', 225: 'brcsta1', 226: 'brican1', 227: 'brcwea1', 228: 'brobab1', 229: 'broman1', 230: 'brosun1', 231: 'brubru1', 232: 'brrwhe3', 233: 'brtcha1', 234: 'brwwar1', 235: 'bswdov1', 236: 'btweye2', 237: 'butapa1', 238: 'bubwar2', 239: 'cabgre1', 240: 'carcha1', 241: 'carwoo1', 242: 'ccbeat1', 243: 'chibat1', 244: 'chewea1', 245: 'chespa1', 246: 'chtapa3', 247: 'chucis1', 248: 'cibwar1', 249: 'cohmar1', 250: 'colsun2', 251: 'combul2', 252: 'combuz1', 253: 'crheag1', 254: 'crefra2', 255: 'crohor1', 256: 'darbar1', 257: 'darter3', 258: 'didcuc1', 259: 'easmog1', 260: 'dutdov1', 261: 'dotbar1', 262: 'edcsun3', 263: 'egygoo', 264: 'eswdov1', 265: 'equaka1', 266: 'eubeat1', 267: 'fatrav1', 268: 'fatwid1', 269: 'fotdro5', 270: 'fislov1', 271: 'gabgos2', 272: 'gbesta1', 273: 'gnbcam2', 274: 'gnhsun1', 275: 'gobbun1', 276: 'grbcam1', 277: 'gobwea1', 278: 'golher1', 279: 'gobsta5', 280: 'grccra1', 281: 'grecor', 282: 'grewoo2', 283: 'grwpyt1', 284: 'gryapa1', 285: 'grywrw1', 286: 'gybfis1', 287: 'gycwar3', 288: 'gyhbus1', 289: 'gyhkin1', 290: 'gyhneg1', 291: 'gyhspa1', 292: 'gytbar1', 293: 'hadibi1', 294: 'hamerk1', 295: 'helgui', 296: 'hartur1', 297: 'hipbab1', 298: 'hunsun2', 299: 'joygre1', 300: 'huncis1', 301: 'kerspa2', 302: 'klacuc1', 303: 'kvbsun1', 304: 'lawgol', 305: 'lessts1', 306: 'lesmaw1', 307: 'libeat1', 308: 'litwea1', 309: 'loceag1', 310: 'lotcor1', 311: 'lotlap1', 312: 'luebus1', 313: 'mabeat1', 314: 'marsto1', 315: 'marsun2', 316: 'malkin1', 317: 'macshr1', 318: 'meypar1', 319: 'mcptit1', 320: 'moccha1', 321: 'mouwag1', 322: 'ndcsun2', 323: 'norbro1', 324: 'nobfly1', 325: 'norcro1', 326: 'norfis1', 327: 'norpuf1', 328: 'nubwoo1', 329: 'pabspa1', 330: 'piecro1', 331: 'palfly2', 332: 'palpri1', 333: 'pitwhy', 334: 'purgre2', 335: 'quailf1', 336: 'pygbat1', 337: 'ratcis1', 338: 'rbsrob1', 339: 'raybar1', 340: 'rebfir2', 341: 'rebhor1', 342: 'reboxp1', 343: 'reccor', 344: 'reccuc1', 345: 'reedov1', 346: 'refcro1', 347: 'refbar2', 348: 'reftin1', 349: 'refwar2', 350: 'reisee2', 351: 'rehwea1', 352: 'rehblu1', 353: 'rewsta1', 354: 'rindov', 355: 'rocmar2', 356: 'rostur1', 357: 'ruegls1', 358: 'sccsun2', 359: 'sacibi2', 360: 'rufcha2', 361: 'scrcha1', 362: 'scthon1', 363: 'sichor1', 364: 'shesta1', 365: 'sincis1', 366: 'slbgre1', 367: 'slcbou1', 368: 'sltnig1', 369: 'sobfly1', 370: 'somgre1', 371: 'somtit4', 372: 'soufis1', 373: 'soucit1', 374: 'spemou2', 375: 'spepig1', 376: 'spewea1', 377: 'spfbar1', 378: 'spfwea1', 379: 'spmthr1', 380: 'spwlap1', 381: 'squher1', 382: 'strsee1', 383: 'subbus1', 384: 'stusta1', 385: 'supsta1', 386: 'tacsun1', 387: 'tafpri1', 388: 'tamdov1', 389: 'thrnig1', 390: 'trobou1', 391: 'varsun2', 392: 'vibsta2', 393: 'vilwea1', 394: 'vimwea1', 395: 'walsta1', 396: 'wbgbir1', 397: 'wbrcha2', 398: 'wbswea1', 399: 'wfbeat1', 400: 'whbcan1', 401: 'whbcou1', 402: 'whbtit5', 403: 'whbcro2', 404: 'whbwea1', 405: 'whbwhe3', 406: 'whcpri2', 407: 'wheslf1', 408: 'whhsaw1', 409: 'whihel1', 410: 'whctur2', 411: 'wlwwar', 412: 'witswa1', 413: 'whrshr1', 414: 'wookin1', 415: 'wtbeat1', 416: 'yebapa1', 417: 'yebbar1', 418: 'yebduc1', 419: 'yebere1', 420: 'yebgre1', 421: 'yeccan1', 422: 'yebsto1', 423: 'yefcan', 424: 'yelbis1', 425: 'yertin1', 426: 'yenspu1', 427: 'yesbar1', 428: 'yespet1', 429: 'yetgre1', 430: 'zebdov', 431: 'afrsil1', 432: 'akepa1', 433: 'akiapo', 434: 'akekee', 435: 'amewig', 436: 'akikik', 437: 'apapan', 438: 'aniani', 439: 'arcter', 440: 'barpet', 441: 'belkin1', 442: 'bkbplo', 443: 'bknsti', 444: 'blkfra', 445: 'bkwpet', 446: 'blknod', 447: 'bongul', 448: 'brant', 449: 'brnboo', 450: 'brnnod', 451: 'brnowl', 452: 'brtcur', 453: 'buffle', 454: 'bulpet', 455: 'bubsan', 456: 'buwtea', 457: 'burpar', 458: 'cacgoo1', 459: 'calqua', 460: 'cangoo', 461: 'canvas', 462: 'caster1', 463: 'chbsan', 464: 'chukar', 465: 'chemun', 466: 'cintea', 467: 'comgal1', 468: 'comwax', 469: 'dunlin', 470: 'coopet', 471: 'crehon', 472: 'eurwig', 473: 'ercfra', 474: 'elepai', 475: 'fragul', 476: 'gadwal', 477: 'gamqua', 478: 'glwgul', 479: 'gnwtea', 480: 'golphe', 481: 'grbher3', 482: 'gresca', 483: 'grefri', 484: 'gwfgoo', 485: 'hawama', 486: 'hawcoo', 487: 'hawcre', 488: 'hawhaw', 489: 'hawpet1', 490: 'hoomer', 491: 'hawgoo', 492: 'houfin', 493: 'iiwi', 494: 'hudgod', 495: 'incter1', 496: 'jabwar', 497: 'japqua', 498: 'kalphe', 499: 'laugul', 500: 'kauama', 501: 'layalb', 502: 'leasan', 503: 'lcspet', 504: 'leater1', 505: 'lesyel', 506: 'lessca', 507: 'lobdow', 508: 'lotjae', 509: 'magpet1', 510: 'madpet', 511: 'mallar3', 512: 'masboo', 513: 'merlin', 514: 'mauala', 515: 'maupar', 516: 'mitpar', 517: 'moudov', 518: 'norcar', 519: 'norhar2', 520: 'normoc', 521: 'norpin', 522: 'norsho', 523: 'oahama', 524: 'omao', 525: 'osprey', 526: 'pagplo', 527: 'palila', 528: 'parjae', 529: 'pecsan', 530: 'peflov', 531: 'perfal', 532: 'pibgre', 533: 'reccar', 534: 'pomjae', 535: 'puaioh', 536: 'redava', 537: 'redjun', 538: 'redpha1', 539: 'refboo', 540: 'rempar', 541: 'rettro', 542: 'ribgul', 543: 'rinduc', 544: 'rinphe', 545: 'rudtur', 546: 'ruff', 547: 'sander', 548: 'semplo', 549: 'sheowl', 550: 'skylar', 551: 'shtsan', 552: 'snogoo', 553: 'sooter1', 554: 'sooshe', 555: 'sora', 556: 'sopsku1', 557: 'sposan', 558: 'towsol', 559: 'wantat1', 560: 'warwhe1', 561: 'wesmea', 562: 'wessan', 563: 'wetshe', 564: 'whfibi', 565: 'whiter', 566: 'whttro', 567: 'wiltur', 568: 'yebcar'}\n",
      "Loading pre-computed mel spectrograms from NPY file, from the path: ../data/processed/pretrain_0422/birdclef2025_melspec_5sec_256_256.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m taxonomy_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mBirdCLEFTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaxonomy_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdatasets_lib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_lib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_lib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mBirdCLEFTrainer.__init__\u001b[0;34m(self, cfg, df, taxonomy_df, datasets_lib, models_lib, learning_lib)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_config()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_label_mapping()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_spectrograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_pseudo_mixup:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pseudo_data()\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mBirdCLEFTrainer._load_spectrograms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_spectrograms\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pre-computed mel spectrograms from NPY file, from the path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mspectrogram_npy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrograms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram_npy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrograms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pre-computed mel spectrograms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:465\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load file containing pickled data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    463\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen allow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to interpret file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m as a pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/numeric.py:1851\u001b[0m, in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m _fromfunction_with_like \u001b[38;5;241m=\u001b[39m array_function_dispatch()(fromfunction)\n\u001b[0;32m-> 1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_frombuffer\u001b[39m(buf, dtype, shape, order):\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(buf, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape(shape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misscalar\u001b[39m(element):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# モデルはmodels_{current_time}に保存される．\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    # taxonomyはラベルとindexの対応を取るために必要．\n",
    "    taxonomy_df = train_df.drop_duplicates(subset=['primary_label']).reset_index(drop=True)\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer = BirdCLEFTrainer(cfg, train_df, taxonomy_df,  datasets_lib, models_lib, learning_lib)\n",
    "    trainer.run()\n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 PyTorch モデル出力比較:\n",
      "最大誤差: 0.0\n",
      "平均誤差: 0.0\n",
      "標準偏差: 0.0\n"
     ]
    }
   ],
   "source": [
    "# モデル出力チェック\n",
    "\n",
    "# モデルパス\n",
    "# 比較元\n",
    "model_1_path = \"../models/epch1_cleaned_0413/model_fold0.pth\"\n",
    "model_2_path = \"../models/models_20250422_1833/model_fold0.pth\"\n",
    "\n",
    "# 共通設定（このcfg_infは必須）\n",
    "cfg_inf = CFG(mode=\"inference\", kaggle_notebook=False)\n",
    "num_classes = train_df['primary_label'].nunique()\n",
    "\n",
    "\n",
    "# モデル読み込み関数\n",
    "def load_model(path):\n",
    "    model = models_lib.BirdCLEFModelForInference(cfg_inf, num_classes)\n",
    "    checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# モデル読み込み\n",
    "model_1 = load_model(model_1_path)\n",
    "model_2 = load_model(model_2_path)\n",
    "\n",
    "# 同じダミー入力\n",
    "dummy_input = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "# 推論（出力に sigmoid が必要な場合は model に含まれてるか確認して適宜追加）\n",
    "with torch.no_grad():\n",
    "    out_0413 = model_1(dummy_input).numpy()\n",
    "    out_0420 = model_2(dummy_input).numpy()\n",
    "\n",
    "# 差分計算\n",
    "abs_diff = np.abs(out_0413 - out_0420)\n",
    "print(\"🔍 PyTorch モデル出力比較:\")\n",
    "print(f\"最大誤差: {np.max(abs_diff)}\")\n",
    "print(f\"平均誤差: {np.mean(abs_diff)}\")\n",
    "print(f\"標準偏差: {np.std(abs_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック1でデバッグできる.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss_1</th>\n",
       "      <th>train_loss_2</th>\n",
       "      <th>val_loss_1</th>\n",
       "      <th>val_loss_2</th>\n",
       "      <th>val_auc_1</th>\n",
       "      <th>val_auc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036685</td>\n",
       "      <td>0.036685</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.801365</td>\n",
       "      <td>0.801365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss_1  train_loss_2  val_loss_1  val_loss_2  val_auc_1  val_auc_2\n",
       "0      0.036685      0.036685    0.025884    0.025884   0.801365   0.801365"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_1_path = \"../models/epch1_cleaned_0413/log_fold0.csv\"\n",
    "log_2_path = \"../models/models_20250422_1826/log_fold0.csv\"\n",
    "\n",
    "log_1 = pd.read_csv(log_1_path)\n",
    "log_2 = pd.read_csv(log_2_path)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"train_loss_1\"] = log_1[\"train_loss\"]\n",
    "df[\"train_loss_2\"] = log_2[\"train_loss\"]\n",
    "\n",
    "df[\"val_loss_1\"] = log_1[\"val_loss\"]\n",
    "df[\"val_loss_2\"] = log_2[\"val_loss\"]\n",
    "\n",
    "df[\"val_auc_1\"] = log_1[\"val_auc\"]\n",
    "df[\"val_auc_2\"] = log_2[\"val_auc\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
