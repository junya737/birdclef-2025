{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN enabled:\", torch.backends.cudnn.enabled)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Tensor device:\", torch.tensor([1.0], device=\"cuda\").device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# å¤§ããªãƒ†ãƒ³ã‚½ãƒ«ã‚’GPUã«è¼‰ã›ã¦è¨ˆç®—\n",
    "x = torch.rand((10000, 10000), device=device)\n",
    "y = torch.rand((10000, 10000), device=device)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    z = torch.matmul(x, y)\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Finished in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# cpu 19sec\n",
    "# cuda 1sec\n",
    "\n",
    "# 100\n",
    "# cuda 9sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY_Mixup(Dataset):\n",
    "    def __init__(self, df, cfg, spectrograms=None, mode=\"train\", label2idx=None, idx2label=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.spectrograms = spectrograms\n",
    "        self.label_to_idx = label2idx\n",
    "        self.idx_to_label = idx2label\n",
    "        self.species_ids = label2idx.keys() if label2idx else []\n",
    "        self.num_classes = len(self.species_ids)\n",
    "\n",
    "        if 'filepath' not in self.df.columns:\n",
    "            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df.filename\n",
    "\n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row1 = self.df.iloc[idx]\n",
    "        spec1 = self._get_spec(row1['samplename'])\n",
    "        label1 = self._get_label(row1)\n",
    "\n",
    "        # === Mixup ===\n",
    "        if self.mode == \"train\" and self.cfg.use_mixup and random.random() < self.cfg.mixup_prob:\n",
    "            idx2 = random.randint(0, len(self.df) - 1)\n",
    "            row2 = self.df.iloc[idx2]\n",
    "            spec2 = self._get_spec(row2['samplename'])\n",
    "            label2 = self._get_label(row2)\n",
    "\n",
    "            lam = np.random.beta(self.cfg.mixup_alpha, self.cfg.mixup_alpha)\n",
    "            spec = lam * spec1 + (1 - lam) * spec2\n",
    "            label = lam * label1 + (1 - lam) * label2\n",
    "        else:\n",
    "            spec = spec1\n",
    "            label = label1\n",
    "\n",
    "        return {\n",
    "            'melspec': spec,\n",
    "            'target': torch.tensor(label, dtype=torch.float32),\n",
    "            'filename': row1['filename']\n",
    "        }\n",
    "\n",
    "    def _get_spec(self, samplename):\n",
    "        if self.spectrograms and samplename in self.spectrograms:\n",
    "            spec = self.spectrograms[samplename]\n",
    "        else:\n",
    "            spec = np.zeros(self.cfg.TARGET_SHAPE, dtype=np.float32)\n",
    "            if self.mode == \"train\":\n",
    "                print(f\"Warning: Spectrogram not found: {samplename}\")\n",
    "\n",
    "        spec = torch.tensor(spec, dtype=torch.float32)\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec.unsqueeze(0)\n",
    "\n",
    "        if self.mode == \"train\" and random.random() < self.cfg.aug_prob:\n",
    "            spec = self.apply_spec_augmentations(spec)\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def _get_label(self, row):\n",
    "        target = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        if row['primary_label'] in self.label_to_idx:\n",
    "            target[self.label_to_idx[row['primary_label']]] = 1.0\n",
    "\n",
    "        if 'secondary_labels' in row and row['secondary_labels'] not in [[''], None, np.nan]:\n",
    "            if isinstance(row['secondary_labels'], str):\n",
    "                secondary_labels = eval(row['secondary_labels'])\n",
    "            else:\n",
    "                secondary_labels = row['secondary_labels']\n",
    "            for label in secondary_labels:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return target\n",
    "\n",
    "    def apply_spec_augmentations(self, spec):\n",
    "        if random.random() < 0.5:\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                width = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[2] - width)\n",
    "                spec[0, :, start:start+width] = 0\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            for _ in range(random.randint(1, 3)):\n",
    "                height = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[1] - height)\n",
    "                spec[0, start:start+height, :] = 0\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            bias = random.uniform(-0.1, 0.1)\n",
    "            spec = spec * gain + bias\n",
    "            spec = torch.clamp(spec, 0, 1)\n",
    "\n",
    "        return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self, mode=\"train\", kaggle_notebook=False, debug=False):\n",
    "        assert mode in [\"train\", \"inference\"], \"mode must be 'train' or 'inference'\"\n",
    "        self.mode = mode\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            \n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.model_path = '/kaggle/input/birdclef-2025-0330' \n",
    "            \n",
    "            # kaggle notebookãªã‚‰ã“ã“ã‚’å¤‰æ›´ã™ã‚‹ï¼\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.spectrogram_npy = '/kaggle/input/birdclef25-mel-spectrograms/birdclef2025_melspec_5sec_256_256.npy'\n",
    "            \n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.RAW_DIR = '../data/raw/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            \n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.models_dir = \"../models/\" # å…¨modelã®ä¿å­˜å…ˆ\n",
    "            self.model_path = self.models_dir # å„ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆï¼å­¦ç¿’æ™‚ã«å‹•çš„ã«å¤‰æ›´ï¼\n",
    "            self.pseudo_label_csv = \"../data/processed/pseudo_labels/ensemble_7sec_pseudoth0.5/pseudo_label.csv\"\n",
    "            self.pseudo_melspec_npy = \"../data/processed/train_soundscapes_0407/train_soundscapes_melspecs.npy\"\n",
    "\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«ãªã‚‰ã“ã“ã‚’å¤‰æ›´ã™ã‚‹ï¼\n",
    "            self.train_csv = '../data/processed/melspec_20250509_1707/train.csv'\n",
    "            self.spectrogram_npy = '../data/processed/melspec_20250509_1707/birdclef2025_melspec_5sec_256_256.npy'\n",
    "\n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = 'efficientnet_b0' # tf_efficientnetv2_b3\n",
    "        self.pretrained = True if mode == \"train\" else False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        \n",
    "        # trainerå†…éƒ¨ã§æ±ºã¾ã‚‹ã®ã§ã“ã“ã§ã¯æŒ‡å®šã—ãªã„ï¼\n",
    "        self.num_classes = None\n",
    "\n",
    "\n",
    "        # ===== Training Mode =====\n",
    "        if mode == \"train\":\n",
    "            self.seed = 42\n",
    "            self.apex = False\n",
    "            self.print_freq = 100\n",
    "            self.num_workers = 2\n",
    "\n",
    "            self.LOAD_DATA = True\n",
    "            self.epochs = 8\n",
    "            self.batch_size = 32\n",
    "            self.criterion = 'BCEWithLogitsLoss'\n",
    "            self.label_smoothing = 0.05\n",
    "\n",
    "            self.n_fold = 5\n",
    "            self.selected_folds = [0]\n",
    "\n",
    "            self.optimizer = 'AdamW'\n",
    "            self.lr = 5e-4\n",
    "            self.weight_decay = 1e-5\n",
    "            self.scheduler = 'CosineAnnealingLR'\n",
    "            self.min_lr = 1e-6\n",
    "            self.T_max = self.epochs\n",
    "            self.full_train = False\n",
    "            self.is_RareFull = False # ãƒ¬ã‚¢ç¨®ã¯å…¨éƒ¨train foldã«ã™ã‚‹\n",
    "            self.aug_prob = 0.5 # spec augmentã®ç¢ºç‡\n",
    "            \n",
    "            # mixupã®è¨­å®š\n",
    "            self.use_mixup = True\n",
    "            self.mixup_alpha = 0.4\n",
    "            self.mixup_prob = 0.8\n",
    "            \n",
    "            \n",
    "            ## ç¾çŠ¶ä½¿ã£ã¦ãªã„ï¼\n",
    "            # self.mixup_alpha_real = 0.5\n",
    "            # self.mixup_alpha_pseudo = 0.5\n",
    "            self.use_pseudo_mixup = False  # pseudo lableã§mixupã™ã‚‹ã‹ã©ã†ã‹\n",
    "            # self.pseudo_mix_prob = 0.4  # mixupã§pseudo lableã‚’ä½¿ã†ç¢ºç‡\n",
    "            # self.pseudo_conf_threshold = 0.5\n",
    "\n",
    "            ###\n",
    "            \n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "            \n",
    "            if self.debug:\n",
    "                self.epochs = 2\n",
    "                self.selected_folds = [0]\n",
    "                self.batch_size = 4\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CFG(mode=\"train\", kaggle_notebook=False, debug=False)\n",
    "\n",
    "if cfg.KAGGLE_NOTEBOOK:\n",
    "    sys.path.append(\"/kaggle/input/birdclef-2025-libs/\")\n",
    "from module import  datasets_lib, models_lib, learning_lib, utils_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainã®å‡¦ç†ã‚’ã‚¯ãƒ©ã‚¹ã§å®Ÿè¡Œï¼\n",
    "class BirdCLEFTrainer:\n",
    "    def __init__(self, cfg, df, taxonomy_df, datasets_lib, models_lib, learning_lib):\n",
    "        self.cfg = cfg\n",
    "        self.df = df.head(100).reset_index(drop=True) if cfg.debug else df\n",
    "        self.taxonomy_df = taxonomy_df\n",
    "        self.datasets_lib = datasets_lib\n",
    "        self.models_lib = models_lib\n",
    "        self.learning_lib = learning_lib\n",
    "        self.spectrograms = None\n",
    "        self.pseudo_df = None\n",
    "        self.pseudo_melspecs = None\n",
    "        self.best_scores = []\n",
    "        self.train_metrics = {}\n",
    "        self.val_metrics = {}\n",
    "        self.label2index = {}\n",
    "        self.index2label = {}\n",
    "        self.num_classes = None\n",
    "\n",
    "        self._setup_model_dir()\n",
    "        self._save_config()\n",
    "        self._build_index_label_mapping()\n",
    "        self._load_spectrograms()\n",
    "        \n",
    "        if self.cfg.use_pseudo_mixup:\n",
    "            self._load_pseudo_data()\n",
    "\n",
    "    def _setup_model_dir(self):\n",
    "        if self.cfg.debug:\n",
    "            current_time = \"debug\"\n",
    "            self.cfg.model_path = os.path.join(self.cfg.models_dir, \"models_debug\")\n",
    "        else:\n",
    "            japan_time = datetime.now(timezone(timedelta(hours=9)))\n",
    "            current_time = japan_time.strftime('%Y%m%d_%H%M')\n",
    "            self.cfg.model_path = os.path.join(self.cfg.models_dir, f\"models_{current_time}\")\n",
    "\n",
    "        os.makedirs(self.cfg.model_path, exist_ok=True)\n",
    "        print(f\"[INFO] Models will be saved to: {self.cfg.model_path}\")\n",
    "\n",
    "        # dataset-metadata.jsonã‚’ä¿å­˜\n",
    "        dataset_metadata = {\n",
    "            \"title\": f\"bc25-models-{current_time}\",\n",
    "            \"id\": f\"ihiratch/bc25-models-{current_time}\",\n",
    "            \"licenses\": [\n",
    "                {\n",
    "                    \"name\": \"CC0-1.0\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        metadata_path = os.path.join(self.cfg.model_path, \"dataset-metadata.json\")\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "    def _save_config(self):\n",
    "        cfg_dict = vars(self.cfg)\n",
    "        cfg_df = pd.DataFrame(list(cfg_dict.items()), columns=[\"key\", \"value\"])\n",
    "        cfg_df.to_csv(os.path.join(self.cfg.model_path, \"config.csv\"), index=False)\n",
    "\n",
    "    def _build_index_label_mapping(self):\n",
    "        species_ids = self.taxonomy_df['primary_label'].tolist()\n",
    "        self.cfg.num_classes = len(species_ids)\n",
    "        # labelã¨indexã®å¯¾å¿œ\n",
    "        self.index2label = {i: label for i, label in enumerate(species_ids)}\n",
    "        self.label2index = {label: i for i, label in enumerate(species_ids)}\n",
    "\n",
    "        print(self.index2label)\n",
    "\n",
    "    def _load_spectrograms(self):\n",
    "        print(f\"Loading pre-computed mel spectrograms from NPY file, from the path: {self.cfg.spectrogram_npy}\")\n",
    "        self.spectrograms = np.load(self.cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "        print(f\"Loaded {len(self.spectrograms)} pre-computed mel spectrograms\")\n",
    "        \n",
    "    def _load_pseudo_data(self):\n",
    "        print(\"ğŸ“¥ Loading pseudo label CSV and melspecs...\")\n",
    "\n",
    "        # row_id ã‚’ index ã«ã—ã¦èª­ã¿è¾¼ã‚€ï¼ˆâ† ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼ï¼‰\n",
    "        self.pseudo_df = pd.read_csv(self.cfg.pseudo_label_csv, index_col=\"row_id\")\n",
    "\n",
    "        # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¾‹: æœ€å¤§å€¤ãŒ 0.5 æœªæº€ã®è¡Œã‚’é™¤ãï¼‰\n",
    "        confidence_threshold = self.cfg.pseudo_conf_threshold\n",
    "        max_probs = self.pseudo_df.max(axis=1)\n",
    "        self.pseudo_df = self.pseudo_df[max_probs > confidence_threshold]\n",
    "        self.pseudo_df = self.pseudo_df.reset_index(drop=False)\n",
    "        print(f\"âœ… Filtered pseudo labels: {len(self.pseudo_df)}\")\n",
    "\n",
    "        # melspec ã¯ key ãŒ row_id ã® dict ã‚’æƒ³å®š\n",
    "        self.pseudo_melspecs = np.load(self.cfg.pseudo_melspec_npy, allow_pickle=True)\n",
    "        print(f\"âœ… Loaded pseudo mel-spectrograms: {len(self.pseudo_melspecs)}\")\n",
    "        \n",
    "    def _create_train_dataset(self, train_df):\n",
    "            return BirdCLEFDatasetFromNPY_Mixup(\n",
    "                    df=train_df,\n",
    "                    cfg=self.cfg,\n",
    "                    spectrograms=self.spectrograms,\n",
    "                    mode=\"train\",\n",
    "                    label2idx=self.label2index,\n",
    "                    idx2label=self.index2label \n",
    "                    )\n",
    "            \n",
    "\n",
    "    def _calculate_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # ğŸ‘‡ ROC AUC ã¯ãƒã‚¤ãƒŠãƒªãƒ©ãƒ™ãƒ«ã‚’å¿…è¦ã¨ã™ã‚‹ã®ã§ã€soft labelã‚’2å€¤åŒ–\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        aucs = [roc_auc_score(targets_bin[:, i], probs[:, i]) \n",
    "                for i in range(targets.shape[1]) if np.sum(targets_bin[:, i]) > 0]\n",
    "        return np.mean(aucs) if aucs else 0.0\n",
    "\n",
    "    def _calculate_classwise_auc(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # ãƒã‚¤ãƒŠãƒªåŒ–ï¼ˆé€£ç¶šå€¤ã§ã‚‚intã§ã‚‚å®‰å…¨ï¼‰\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_auc = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_auc[i] = roc_auc_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_auc[i] = np.nan  # ã‚¨ãƒ©ãƒ¼å‡ºãŸã¨ãã‚‚å®‰å¿ƒ\n",
    "        return classwise_auc\n",
    "\n",
    "    def _calculate_classwise_ap(self, targets, outputs):\n",
    "        probs = 1 / (1 + np.exp(-outputs))\n",
    "\n",
    "        # ãƒ©ãƒ™ãƒ«ã‚’ãƒã‚¤ãƒŠãƒªåŒ–ï¼ˆsoft labelå¯¾å¿œï¼‰\n",
    "        targets_bin = (targets >= 0.5).astype(int)\n",
    "\n",
    "        classwise_ap = {}\n",
    "        for i in range(targets.shape[1]):\n",
    "            if np.sum(targets_bin[:, i]) > 0:\n",
    "                try:\n",
    "                    classwise_ap[i] = average_precision_score(targets_bin[:, i], probs[:, i])\n",
    "                except ValueError:\n",
    "                    classwise_ap[i] = np.nan\n",
    "        return classwise_ap\n",
    "    \n",
    "    def _calculate_map(self, targets, outputs):\n",
    "        classwise_ap = self._calculate_classwise_ap(targets, outputs)\n",
    "        values = [v for v in classwise_ap.values() if v is not None and not np.isnan(v)]\n",
    "        return np.mean(values) if values else 0.0\n",
    "\n",
    "    def _save_classwise_scores_to_csv(self, classwise_auc, classwise_ap, fold, filename_prefix):\n",
    "        rows = []\n",
    "        for i in classwise_auc:\n",
    "            label = self.index2label.get(i, str(i))\n",
    "            auc = classwise_auc[i]\n",
    "            ap = classwise_ap.get(i, np.nan)\n",
    "            rows.append({\"label\": label, \"val_auc\": auc, \"val_ap\": ap})\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(os.path.join(self.cfg.model_path, f\"{filename_prefix}_classwise_score_fold{fold}.csv\"), index=False)\n",
    "\n",
    "\n",
    "    def train_one_epoch(self, model, loader, optimizer, criterion, device, scheduler=None):\n",
    "        model.train()\n",
    "        losses, all_targets, all_outputs = [], [], []\n",
    "\n",
    "        pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "        for step, batch in pbar:\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs, batch_losses = [], []\n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                optimizer.step()\n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = outputs[1] if isinstance(outputs, tuple) else criterion(outputs, targets)\n",
    "                outputs = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            if scheduler and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss.item() if not isinstance(loss, float) else loss)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        self.train_metrics = {\n",
    "            'train_loss': np.mean(losses),\n",
    "            'train_auc': self._calculate_auc(all_targets, all_outputs),\n",
    "            \"train_map\": self._calculate_map(all_targets, all_outputs),   \n",
    "            \"train_classwise_auc\": self._calculate_classwise_auc(all_targets, all_outputs),\n",
    "            \"train_classwise_ap\": self._calculate_classwise_ap(all_targets, all_outputs),  \n",
    "        }\n",
    "\n",
    "    def validate(self, model, loader, criterion, device):\n",
    "        model.eval()\n",
    "        losses, all_targets, all_outputs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Validation\"):\n",
    "                if isinstance(batch['melspec'], list):\n",
    "                    batch_outputs, batch_losses = [], []\n",
    "                    for i in range(len(batch['melspec'])):\n",
    "                        inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                        target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                        output = model(inputs)\n",
    "                        loss = criterion(output, target)\n",
    "                        batch_outputs.append(output.detach().cpu())\n",
    "                        batch_losses.append(loss.item())\n",
    "                    outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                    loss = np.mean(batch_losses)\n",
    "                    targets = batch['target'].numpy()\n",
    "                else:\n",
    "                    inputs = batch['melspec'].to(device)\n",
    "                    targets = batch['target'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    outputs = outputs.detach().cpu().numpy()\n",
    "                    targets = targets.detach().cpu().numpy()\n",
    "\n",
    "                all_outputs.append(outputs)\n",
    "                all_targets.append(targets)\n",
    "                losses.append(loss.item() if not isinstance(loss, float) else loss)\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        # print(\"Size of validation:\",  len(all_targets))\n",
    "        self.val_metrics = {\n",
    "            'val_loss': np.mean(losses),\n",
    "            'val_auc': self._calculate_auc(all_targets, all_outputs),\n",
    "            \"val_map\": self._calculate_map(all_targets, all_outputs),\n",
    "            \"val_classwise_auc\": self._calculate_classwise_auc(all_targets, all_outputs),\n",
    "            \"val_classwise_ap\": self._calculate_classwise_ap(all_targets, all_outputs),\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        for fold in range(self.cfg.n_fold):\n",
    "            if fold not in self.cfg.selected_folds:\n",
    "                continue\n",
    "            print(f\"\\n{'='*30} Fold {fold} {'='*30}\")\n",
    "\n",
    "            # train.csvã®foldã‚’ä½¿ã†ï¼\n",
    "            \n",
    "            if self.cfg.full_train:\n",
    "                train_df = self.df.reset_index(drop=True)\n",
    "                val_df = self.df[self.df['fold'] == fold].reset_index(drop=True)\n",
    "                print(\"Use full train data for training.\")\n",
    "            else:\n",
    "                train_df = self.df[self.df['fold'] != fold].reset_index(drop=True)\n",
    "                val_df = self.df[self.df['fold'] == fold].reset_index(drop=True) \n",
    "            \n",
    "            print(f\"Training set: {len(train_df)} samples\")\n",
    "            print(f\"Validation set: {len(val_df)} samples\")\n",
    "\n",
    "            train_dataset = self._create_train_dataset(train_df)\n",
    "            val_dataset = BirdCLEFDatasetFromNPY_Mixup(\n",
    "                        df=val_df,\n",
    "                        cfg=self.cfg,\n",
    "                        spectrograms=self.spectrograms,\n",
    "                        mode='valid',\n",
    "                        label2idx=self.label2index,\n",
    "                        idx2label=self.index2label\n",
    "                    )\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.cfg.batch_size, shuffle=True, \n",
    "                                       num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "                                       collate_fn=self.datasets_lib.collate_fn, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "                                     num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "                                     collate_fn=self.datasets_lib.collate_fn)\n",
    "\n",
    "            model = self.models_lib.BirdCLEFModelForTrain(self.cfg).to(self.cfg.device)\n",
    "            optimizer = self.learning_lib.get_optimizer(model, self.cfg)\n",
    "            criterion = self.learning_lib.get_criterion(self.cfg)\n",
    "\n",
    "            scheduler = (lr_scheduler.OneCycleLR(optimizer, max_lr=self.cfg.lr, \n",
    "                        steps_per_epoch=len(train_loader), epochs=self.cfg.epochs, pct_start=0.1)\n",
    "                         if self.cfg.scheduler == 'OneCycleLR'\n",
    "                         else self.learning_lib.get_scheduler(optimizer, self.cfg))\n",
    "\n",
    "            best_auc = 0\n",
    "            log_history = []\n",
    "\n",
    "            for epoch in range(self.cfg.epochs):\n",
    "                print(f\"\\nEpoch {epoch+1}/{self.cfg.epochs}\")\n",
    "                start_time = time.time()\n",
    "\n",
    "                self.train_one_epoch(model, train_loader, optimizer, criterion, self.cfg.device, scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None)\n",
    "                self.validate(model, val_loader, criterion, self.cfg.device)\n",
    "\n",
    "                # ã‚¹ã‚³ã‚¢å–å¾—\n",
    "                train_loss = self.train_metrics['train_loss']\n",
    "                train_auc = self.train_metrics['train_auc']\n",
    "                train_auc_map = self.train_metrics['train_map']\n",
    "\n",
    "                val_loss = self.val_metrics['val_loss']\n",
    "                val_auc = self.val_metrics['val_auc']\n",
    "                val_auc_map = self.val_metrics['val_map']\n",
    "                val_classwise_auc = self.val_metrics['val_classwise_auc']\n",
    "                val_classwise_ap = self.val_metrics['val_classwise_ap']\n",
    "\n",
    "                if scheduler and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step(val_loss if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau) else None)\n",
    "\n",
    "                print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}, Train MAP: {train_auc_map:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val MAP: {val_auc_map:.4f}\")\n",
    "\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    print(f\"New best AUC: {best_auc:.4f} at epoch {epoch+1}\")\n",
    "                    \n",
    "                    self._save_classwise_scores_to_csv(val_classwise_auc, val_classwise_ap, fold, filename_prefix=\"best_val\")\n",
    "\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'epoch': epoch,\n",
    "                        'val_auc': val_auc,\n",
    "                        'train_auc': train_auc,\n",
    "                        \"index2label\": self.index2label,\n",
    "                        'cfg': self.cfg\n",
    "                    }, f\"{self.cfg.model_path}/model_fold{fold}.pth\")\n",
    "\n",
    "                log_entry = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'lr': scheduler.get_last_lr()[0] if scheduler else self.cfg.lr,\n",
    "                    'epoch_time_min': round((time.time() - start_time) / 60, 2)\n",
    "                }\n",
    "\n",
    "                # classwiseã‚¹ã‚³ã‚¢ã‚’é™¤å¤–ã—ãŸ val_metrics ã®ãƒ­ã‚°\n",
    "                train_log = {f\"{k}\": v for k, v in self.train_metrics.items() if not k.startswith(\"train_classwise\")}\n",
    "                val_log = {f\"{k}\": v for k, v in self.val_metrics.items() if not k.startswith(\"val_classwise\")}\n",
    "                \n",
    "                # ãƒ­ã‚°ç”¨ã‚¹ã‚³ã‚¢ã®æ›´æ–°ï¼ˆclasswiseã¯é™¤å¤–ï¼‰\n",
    "                log_entry.update(train_log)\n",
    "                log_entry.update(val_log)\n",
    "                log_history.append(log_entry)\n",
    "            \n",
    "           \n",
    "                \n",
    "\n",
    "            pd.DataFrame(log_history).to_csv(f\"{self.cfg.model_path}/log_fold{fold}.csv\", index=False)\n",
    "            self.best_scores.append(best_auc)\n",
    "            print(f\"\\nBest AUC for fold {fold}: {best_auc:.4f}\")\n",
    "\n",
    "            del model, optimizer, scheduler, train_loader, val_loader\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Cross-Validation Results:\")\n",
    "        for fold, score in enumerate(self.best_scores):\n",
    "            print(f\"Fold {self.cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "        print(f\"Mean AUC: {np.mean(self.best_scores):.4f}\")\n",
    "        print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¬ã‚¢ç¨®ã¯fold=-1ã«ã™ã‚‹ï¼\n",
    "def overwrite_fold_for_rare_classes(df, rare_threshold=5):\n",
    "    # å„ãƒ©ãƒ™ãƒ«ã®å‡ºç¾æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    label_counts = df.groupby('primary_label').size()\n",
    "\n",
    "    # rareãªãƒ©ãƒ™ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    rare_labels = label_counts[label_counts < rare_threshold].index.tolist()\n",
    "\n",
    "    print(f\"Rare labels ({len(rare_labels)} classes): {rare_labels[:10]}{'...' if len(rare_labels) > 10 else ''}\")\n",
    "\n",
    "    # rareãªãƒ©ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ fold = -1 ã«ä¸Šæ›¸ã\n",
    "    df.loc[df['primary_label'].isin(rare_labels), 'fold'] = -1\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã¯models_{current_time}ã«ä¿å­˜ã•ã‚Œã‚‹ï¼\n",
    "if __name__ == \"__main__\":\n",
    "    utils_lib.set_seed(cfg.seed)\n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    \n",
    "    if cfg.is_RareFull: \n",
    "        train_df = overwrite_fold_for_rare_classes(train_df, rare_threshold=5)\n",
    "        \n",
    "    # taxonomyã¯ãƒ©ãƒ™ãƒ«ã¨indexã®å¯¾å¿œã‚’å–ã‚‹ãŸã‚ã«å¿…è¦ï¼\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer = BirdCLEFTrainer(cfg, train_df, taxonomy_df,  datasets_lib, models_lib, learning_lib)\n",
    "    trainer.run()\n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir_name = \"models_20250505_2235\" # ã“ã“ã‚’å¤‰æ›´\n",
    "model_dir = os.path.join(cfg.models_dir, model_dir_name)\n",
    "\n",
    "# ã‚¹ã‚³ã‚¢æ ¼ç´è¾æ›¸ï¼ˆfoldã”ã¨ã®è¨˜éŒ²ï¼‰\n",
    "score_lists = {\n",
    "    'val_auc': [],\n",
    "    'train_auc': [],\n",
    "    'val_map': [],\n",
    "    'train_map': [],\n",
    "    'val_loss': [],\n",
    "    'train_loss': [],\n",
    "    'epoch': [],\n",
    "}\n",
    "\n",
    "# å„foldã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢åé›†\n",
    "for fold in range(5):\n",
    "    log_path = os.path.join(model_dir, f\"log_fold{fold}.csv\")\n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"Missing log for fold {fold}: {log_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(log_path)\n",
    "    best_row = df.loc[df['val_auc'].idxmax()]\n",
    "\n",
    "    print(f\"Fold {fold} best epoch: {int(best_row['epoch'])}, val_auc: {best_row['val_auc']:.3f}, train_auc: {best_row['train_auc']:.3f}\")\n",
    "\n",
    "    for key in score_lists:\n",
    "        score_lists[key].append(best_row[key])\n",
    "\n",
    "# å¹³å‡ã‚¹ã‚³ã‚¢ã‚’æ•´å½¢ï¼ˆ.3fã§è¡¨ç¤ºã€epochã ã‘.2fï¼‰\n",
    "score_means = {}\n",
    "for key, values in score_lists.items():\n",
    "    avg = sum(values) / len(values)\n",
    "    display_key = f\"Avg {key.replace('_', ' ').title()}\"\n",
    "    if \"epoch\" in key:\n",
    "        score_means[display_key] = f\"{avg:.2f}\"\n",
    "    else:\n",
    "        score_means[display_key] = f\"{avg:.3f}\"\n",
    "\n",
    "# config.csv èª­ã¿è¾¼ã¿\n",
    "config_path = os.path.join(model_dir, \"config.csv\")\n",
    "config_df = pd.read_csv(config_path)\n",
    "\n",
    "important_keys = [\n",
    "    'model_name','batch_size', 'epochs',\n",
    "    'optimizer', 'lr', 'weight_decay', 'scheduler', 'min_lr', \"tta\",\n",
    "]\n",
    "\n",
    "# configæƒ…å ±ã®çµ±åˆ\n",
    "config_dict = {\"Note\": \"\", \"LB AUC\": \"\", **score_means }\n",
    "for key in important_keys:\n",
    "    value = config_df.loc[config_df['key'] == key, 'value'].values\n",
    "    config_dict[key] = value[0] if len(value) > 0 else \"\"\n",
    "\n",
    "# Markdownå‡ºåŠ›\n",
    "all_keys = list(config_dict.keys())\n",
    "print(\"\\n```markdown\")\n",
    "print(\"| \" + \" | \".join(all_keys) + \" |\")\n",
    "print(\"|\" + \"|\".join([\"-\" * (len(k)+2) for k in all_keys]) + \"|\")\n",
    "print(\"| \" + \" | \".join(str(config_dict[k]) for k in all_keys) + \" |\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_name = \"fold0_safezone1000_mixup0.5\"\n",
    "i = 0\n",
    "model_dir = os.path.join(cfg.models_dir, model_dir_name)\n",
    "log_path = os.path.join(model_dir, f\"log_fold{i}.csv\")\n",
    "\n",
    "# lossã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "df = pd.read_csv(log_path)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', color='blue')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹\n",
    "# æ¯”è¼ƒå…ƒ\n",
    "model_1_path = \"../models/fold0_RemVoice0426_epch6//model_fold0.pth\"\n",
    "model_2_path = \"../models/fold0_RemVoiceMan0426_epch6//model_fold0.pth\"\n",
    "\n",
    "# å…±é€šè¨­å®šï¼ˆã“ã®cfg_infã¯å¿…é ˆï¼‰\n",
    "cfg_inf = CFG(mode=\"inference\", kaggle_notebook=False)\n",
    "num_classes = train_df['primary_label'].nunique()\n",
    "\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°\n",
    "def load_model(path):\n",
    "    model = models_lib.BirdCLEFModelForInference(cfg_inf, num_classes)\n",
    "    checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "model_1 = load_model(model_1_path)\n",
    "model_2 = load_model(model_2_path)\n",
    "\n",
    "# åŒã˜ãƒ€ãƒŸãƒ¼å…¥åŠ›\n",
    "dummy_input = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "# æ¨è«–ï¼ˆå‡ºåŠ›ã« sigmoid ãŒå¿…è¦ãªå ´åˆã¯ model ã«å«ã¾ã‚Œã¦ã‚‹ã‹ç¢ºèªã—ã¦é©å®œè¿½åŠ ï¼‰\n",
    "with torch.no_grad():\n",
    "    out_0413 = model_1(dummy_input).numpy()\n",
    "    out_0420 = model_2(dummy_input).numpy()\n",
    "\n",
    "# å·®åˆ†è¨ˆç®—\n",
    "abs_diff = np.abs(out_0413 - out_0420)\n",
    "print(\"ğŸ” PyTorch ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›æ¯”è¼ƒ:\")\n",
    "print(f\"æœ€å¤§èª¤å·®: {np.max(abs_diff)}\")\n",
    "print(f\"å¹³å‡èª¤å·®: {np.mean(abs_diff)}\")\n",
    "print(f\"æ¨™æº–åå·®: {np.std(abs_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_1_path = \"../models/epch1_cleaned_0413/log_fold0.csv\"\n",
    "log_2_path = \"../models/models_20250422_1826/log_fold0.csv\"\n",
    "\n",
    "log_1 = pd.read_csv(log_1_path)\n",
    "log_2 = pd.read_csv(log_2_path)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"train_loss_1\"] = log_1[\"train_loss\"]\n",
    "df[\"train_loss_2\"] = log_2[\"train_loss\"]\n",
    "\n",
    "df[\"val_loss_1\"] = log_1[\"val_loss\"]\n",
    "df[\"val_loss_2\"] = log_2[\"val_loss\"]\n",
    "\n",
    "df[\"val_auc_1\"] = log_1[\"val_auc\"]\n",
    "df[\"val_auc_2\"] = log_2[\"val_auc\"]\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
