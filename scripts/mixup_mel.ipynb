{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BirdCLEF 2025 Data Preprocessing Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:10:40.81525Z",
     "iopub.status.busy": "2025-03-17T13:10:40.814873Z",
     "iopub.status.idle": "2025-03-17T13:10:45.829114Z",
     "shell.execute_reply": "2025-03-17T13:10:45.828024Z",
     "shell.execute_reply.started": "2025-03-17T13:10:40.815215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from module import config_lib, utils_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    def __init__(self, kaggle_notebook=False, debug=False):\n",
    "        self.KAGGLE_NOTEBOOK = kaggle_notebook\n",
    "        self.debug = debug\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        if self.KAGGLE_NOTEBOOK:\n",
    "            self.OUTPUT_DIR = ''\n",
    "            self.train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "            self.train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "            self.test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "            self.submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "            self.taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "            self.model_path = '/kaggle/input/birdclef-2025-0330'\n",
    "        else:\n",
    "            self.OUTPUT_DIR = '../data/result/'\n",
    "            self.train_datadir = '../data/raw/train_audio/'\n",
    "            self.test_soundscapes = '../data/raw/test_soundscapes/'\n",
    "            self.submission_csv = '../data/raw/sample_submission.csv'\n",
    "            self.taxonomy_csv = '../data/raw/taxonomy.csv'\n",
    "            self.models_dir = \"../models/\" # 全modelの保存先\n",
    "            self.model_path = self.models_dir # 各モデルの保存先．学習時に動的に変更．\n",
    "            self.RAW_DIR = '../data/raw/'\n",
    "            self.PROCESSED_DIR = '../data/processed/'\n",
    "            \n",
    "            self.train_csv = '../data/processed/mel_CropAugment/train.csv'\n",
    "            self.spectrogram_npy = '../data/processed/mel_CropAugment/birdclef2025_melspec_5sec_256_256.npy'\n",
    "\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.FS = 32000\n",
    "        self.WINDOW_SIZE = 5.0 # 推論時のウィンドウサイズ\n",
    "        self.TARGET_DURATION = 5 # データセット作成時のウィンドウサイズ\n",
    "        self.TARGET_SHAPE = (256, 256)\n",
    "        self.N_FFT = 1024\n",
    "        self.HOP_LENGTH = 16\n",
    "        self.N_MELS = 148\n",
    "        self.FMIN = 20\n",
    "        self.FMAX = 16000\n",
    "        self.N_MAX = 50 if self.debug else None        \n",
    "        self.N_JOBS = 16  # 並列処理のスレッド数 16くらいでいい\n",
    "        self.LOAD_ENGINE = 'torchaudio'  # librosa or torchaudio\n",
    "        self.SKIP_RESIZE = False  # resizeしないならTrue\n",
    "        self.seed = 42\n",
    "        self.n_fold = 5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:15:45.762845Z",
     "iopub.status.busy": "2025-03-17T13:15:45.762471Z",
     "iopub.status.idle": "2025-03-17T13:15:45.768405Z",
     "shell.execute_reply": "2025-03-17T13:15:45.766979Z",
     "shell.execute_reply.started": "2025-03-17T13:15:45.762812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = DatasetConfig(kaggle_notebook=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_lib.set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T13:16:15.414418Z",
     "iopub.status.busy": "2025-03-17T13:16:15.414035Z",
     "iopub.status.idle": "2025-03-17T13:16:15.55526Z",
     "shell.execute_reply": "2025-03-17T13:16:15.553984Z",
     "shell.execute_reply.started": "2025-03-17T13:16:15.414356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: OFF\n",
      "Max samples to process: ALL\n",
      "Loading taxonomy data...\n",
      "Loading training metadata...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Debug mode: {'ON' if cfg.debug else 'OFF'}\")\n",
    "print(f\"Max samples to process: {cfg.N_MAX if cfg.N_MAX is not None else 'ALL'}\")\n",
    "\n",
    "print(\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(f'{cfg.RAW_DIR}/taxonomy.csv')\n",
    "species_class_map = dict(zip(taxonomy_df['primary_label'], taxonomy_df['class_name']))\n",
    "\n",
    "print(\"Loading training metadata...\")\n",
    "train_df = pd.read_csv(f'{cfg.RAW_DIR}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df_aug = pd.read_csv(cfg.train_csv)\n",
    "# mixupの有無を示すカラムを追加\n",
    "if 'is_mixup' not in working_df_aug.columns:\n",
    "    working_df_aug['is_mixup'] = False\n",
    "if 'mix_partner' not in working_df_aug.columns:\n",
    "    working_df_aug['mix_partner'] = None\n",
    "if 'mixup_weight' not in working_df_aug.columns:\n",
    "    working_df_aug['mixup_weight'] = None\n",
    "\n",
    "# === 2. Rare label 抽出（例: <50件） ===\n",
    "label_counts = working_df_aug['primary_label'].value_counts()\n",
    "# ✅ レア種候補の primary_label を抽出\n",
    "rare_labels = working_df_aug.loc[working_df_aug['n_augment'] > 0, 'primary_label'].unique().tolist()\n",
    "\n",
    "# ✅ そのうちの crop だけを対象に\n",
    "rare_crop_df = working_df_aug[\n",
    "    (working_df_aug['primary_label'].isin(rare_labels)) &\n",
    "    (working_df_aug['samplename'].str.contains('_crop')) &\n",
    "    (~working_df_aug['samplename'].str.contains('_mix_'))  # 既存mixup除外\n",
    "]\n",
    "\n",
    "# === ラベル情報 ===\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(working_df_aug['primary_label'].unique()))}\n",
    "num_classes = len(label2id)\n",
    "\n",
    "# === レア種crop（augmentationされた）だけを抽出 ===\n",
    "rare_crop_df = working_df_aug[\n",
    "    (working_df_aug['n_augment'] > 0) &\n",
    "    (working_df_aug['samplename'].str.contains('_crop')) &\n",
    "    (~working_df_aug['samplename'].str.contains('_mix_'))\n",
    "].copy()\n",
    "\n",
    "# === コモン種抽出 ===\n",
    "num_common_labels = 50 # 50位でデータ数180程度．\n",
    "label_counts = working_df_aug['primary_label'].value_counts()\n",
    "common_labels = label_counts.head(num_common_labels).index.tolist()\n",
    "\n",
    "common_df = working_df_aug[working_df_aug['primary_label'].isin(common_labels)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レア種にコモン種を混ぜて上書き．cropされているものはすべてmixup対象\n",
    "all_bird_data = np.load(cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "\n",
    "for idx, row in rare_crop_df.iterrows():\n",
    "    s_rare = row['samplename']\n",
    "    label = row['primary_label']\n",
    "    if s_rare not in all_bird_data:\n",
    "        continue\n",
    "\n",
    "    # === コモン種のcropを1つランダムに選択 乱数固定\n",
    "    s_common_row = common_df.sample(1, random_state=cfg.seed + idx).iloc[0]\n",
    "    s_common = s_common_row['samplename']\n",
    "    common_label = s_common_row['primary_label']\n",
    "\n",
    "    if s_common not in all_bird_data:\n",
    "        continue\n",
    "\n",
    "    # === Mixup実行 ===\n",
    "    m1 = all_bird_data[s_rare]\n",
    "    m2 = all_bird_data[s_common]\n",
    "\n",
    "    lam = np.random.uniform(0.7, 0.9)\n",
    "    mixed = np.clip(lam * m1 + (1 - lam) * m2, 0, 1)\n",
    "\n",
    "    # === melを上書き ===\n",
    "    all_bird_data[s_rare] = mixed\n",
    "\n",
    "    # === working_dfの該当行を更新 ===\n",
    "    working_df_aug.loc[idx, 'is_mixup'] = True\n",
    "    working_df_aug.loc[idx, 'mix_partner'] = s_common\n",
    "    working_df_aug.loc[idx, 'mixup_weight'] = round(float(lam), 4)\n",
    "\n",
    "    # === secondary_labels を追加・更新 ===\n",
    "    try:\n",
    "        current_sec = row['secondary_labels']\n",
    "        current_sec_list = eval(current_sec) if isinstance(current_sec, str) else []\n",
    "    except:\n",
    "        current_sec_list = []\n",
    "\n",
    "    # --- 型・形式の安全処理 ---\n",
    "    if not isinstance(current_sec_list, list):\n",
    "        current_sec_list = []\n",
    "\n",
    "    # ✅ 空文字やNoneを除去\n",
    "    current_sec_list = [s for s in current_sec_list if s and s.strip() != '']\n",
    "\n",
    "    # ✅ コモン種ラベルが入っていなければ追加\n",
    "    if common_label not in current_sec_list:\n",
    "        current_sec_list.append(common_label)\n",
    "\n",
    "    working_df_aug.loc[idx, 'secondary_labels'] = str(current_sec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Mel-spectrograms saved to: ../data/processed/melspec_20250412_1803/birdclef2025_melspec_5sec_256_256.npy\n",
      "📦 File size: 7980.93 MB\n",
      "📐 Example shape: (256, 256)\n",
      "📝 Config saved to: ../data/processed/melspec_20250412_1803/config.csv\n",
      "📝 Augmented training metadata saved to: ../data/processed/melspec_20250412_1803/train.csv\n",
      "📊 Total rows: 31913\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# === JST時刻でディレクトリ作成 ===\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "now = datetime.now(jst)\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# ✅ 保存先フォルダを debug に応じて分岐\n",
    "if cfg.debug:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, \"data_debugs\")\n",
    "else:\n",
    "    output_dir = os.path.join(cfg.PROCESSED_DIR, f\"melspec_{timestamp}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 1. melスペクトログラム（all_bird_data）の保存 ===\n",
    "output_path = os.path.join(output_dir, \"birdclef2025_melspec_5sec_256_256.npy\")\n",
    "\n",
    "wrapped_array = np.array(all_bird_data, dtype=object)\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(wrapped_array, f, protocol=5)\n",
    "\n",
    "print(f\"\\n✅ Mel-spectrograms saved to: {output_path}\")\n",
    "print(f\"📦 File size: {os.path.getsize(output_path) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"📐 Example shape: {next(iter(all_bird_data.values())).shape}\")\n",
    "\n",
    "# === 2. configの保存 ===\n",
    "config_path = os.path.join(output_dir, \"config.csv\")\n",
    "config_dict = {k: v for k, v in vars(cfg).items() if not k.startswith(\"__\")}\n",
    "\n",
    "with open(config_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"key\", \"value\"])\n",
    "    for key, value in config_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"📝 Config saved to: {config_path}\")\n",
    "\n",
    "# === 3. augmented metadata の保存 ===\n",
    "train_csv_path = os.path.join(output_dir, \"train.csv\")\n",
    "working_df_aug.to_csv(train_csv_path, index=False)\n",
    "\n",
    "print(f\"📝 Augmented training metadata saved to: {train_csv_path}\")\n",
    "print(f\"📊 Total rows: {len(working_df_aug)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
