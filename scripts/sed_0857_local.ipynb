{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0c80c0",
   "metadata": {
    "papermill": {
     "duration": 0.004337,
     "end_time": "2025-05-20T09:22:44.781903",
     "exception": false,
     "start_time": "2025-05-20T09:22:44.777566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Some useful references:\n",
    "1. **[Training]**: https://github.com/LIHANG-HONG/birdclef2023-2nd-place-solution\n",
    "2. **[Inference]**: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25\n",
    "\n",
    "This model backbone is seresnext26t_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b9a4afbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:22:44.790653Z",
     "iopub.status.busy": "2025-05-20T09:22:44.790232Z",
     "iopub.status.idle": "2025-05-20T09:23:07.830054Z",
     "shell.execute_reply": "2025-05-20T09:23:07.829008Z"
    },
    "papermill": {
     "duration": 23.046365,
     "end_time": "2025-05-20T09:23:07.831968",
     "exception": false,
     "start_time": "2025-05-20T09:22:44.785603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e8c9526f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.840787Z",
     "iopub.status.busy": "2025-05-20T09:23:07.840278Z",
     "iopub.status.idle": "2025-05-20T09:23:07.846257Z",
     "shell.execute_reply": "2025-05-20T09:23:07.845295Z"
    },
    "papermill": {
     "duration": 0.012329,
     "end_time": "2025-05-20T09:23:07.848146",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.835817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self, mode=\"inference\"):\n",
    "        self.seed = 42\n",
    "        self.print_freq = 100\n",
    "        self.num_workers = 4\n",
    "        self.stage = \"train_bce\"\n",
    "        self.mode = mode  # \"train\" or \"inference\"\n",
    "\n",
    "        # ===== Path Settings =====\n",
    "        self.train_datadir = \"/kaggle/input/birdclef-2025/train_audio\"\n",
    "        self.train_csv = \"/kaggle/input/birdclef-2025/train.csv\"\n",
    "        self.test_soundscapes = \"../data/raw/test_soundscapes_small/\"\n",
    "        self.submission_csv = \"../data/raw/sample_submission.csv\"\n",
    "        self.taxonomy_csv = \"../data/raw/taxonomy.csv\"\n",
    "        self.model_files = [\"../models/sedmodel_0857/model_fold0.pth\"]\n",
    "\n",
    "        # ===== Model Settings =====\n",
    "        self.model_name = \"seresnext26t_32x4d\"\n",
    "        self.pretrained = False\n",
    "        self.in_channels = 1\n",
    "\n",
    "        # ===== Audio Settings =====\n",
    "        self.SR = 32000\n",
    "        self.target_duration = 5\n",
    "        self.train_duration = 10\n",
    "        \n",
    "        cfg.n_mels = 128\n",
    "        cfg.n_fft = 2048\n",
    "        cfg.hop_length = 512\n",
    "        cfg.f_min = 20\n",
    "        cfg.f_max = 16000\n",
    "        cfg.normal = 80  # or 255\n",
    "        cfg.infer_duration = 5\n",
    "        cfg.duration_train = 10\n",
    "\n",
    "        # ===== Device =====\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a00ddcf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.856015Z",
     "iopub.status.busy": "2025-05-20T09:23:07.855714Z",
     "iopub.status.idle": "2025-05-20T09:23:07.884060Z",
     "shell.execute_reply": "2025-05-20T09:23:07.882969Z"
    },
    "papermill": {
     "duration": 0.034235,
     "end_time": "2025-05-20T09:23:07.885805",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.851570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "760ede7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.894442Z",
     "iopub.status.busy": "2025-05-20T09:23:07.894078Z",
     "iopub.status.idle": "2025-05-20T09:23:07.908823Z",
     "shell.execute_reply": "2025-05-20T09:23:07.907782Z"
    },
    "papermill": {
     "duration": 0.021291,
     "end_time": "2025-05-20T09:23:07.910633",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.889342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "62d418cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.919175Z",
     "iopub.status.busy": "2025-05-20T09:23:07.918849Z",
     "iopub.status.idle": "2025-05-20T09:23:07.928299Z",
     "shell.execute_reply": "2025-05-20T09:23:07.927410Z"
    },
    "papermill": {
     "duration": 0.015713,
     "end_time": "2025-05-20T09:23:07.929967",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.914254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9ef99c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.938815Z",
     "iopub.status.busy": "2025-05-20T09:23:07.938451Z",
     "iopub.status.idle": "2025-05-20T09:23:07.960446Z",
     "shell.execute_reply": "2025-05-20T09:23:07.959563Z"
    },
    "papermill": {
     "duration": 0.028016,
     "end_time": "2025-05-20T09:23:07.962043",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.934027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg.n_mels)\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in cfg.model_name:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=cfg.SR,\n",
    "            hop_length=cfg.hop_length,\n",
    "            n_mels=cfg.n_mels,\n",
    "            f_min=cfg.f_min,\n",
    "            f_max=cfg.f_max,\n",
    "            n_fft=cfg.n_fft,\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if cfg.device == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "        audio = audio.float()\n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg.normal == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg.normal == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg.in_channels == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        clipwise_output, norm_att, segmentwise_output = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x, _ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg.infer_duration / self.cfg.duration_train) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg.infer_duration / self.cfg.duration_train)\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start, end, x, time_att)\n",
    "\n",
    "        start_minus = max(0, start - tta_delta)\n",
    "        end_minus = end - tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus, end_minus, x, time_att)\n",
    "\n",
    "        start_plus = start + tta_delta\n",
    "        end_plus = min(feat_time, end + tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus, end_plus, x, time_att)\n",
    "\n",
    "        pred = 0.5 * pred + 0.25 * pred_minus + 0.25 * pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self, start, end, x, time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c08af6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.970489Z",
     "iopub.status.busy": "2025-05-20T09:23:07.970172Z",
     "iopub.status.idle": "2025-05-20T09:23:07.981960Z",
     "shell.execute_reply": "2025-05-20T09:23:07.980629Z"
    },
    "papermill": {
     "duration": 0.018091,
     "end_time": "2025-05-20T09:23:07.983822",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.965731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "032dd269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:07.991979Z",
     "iopub.status.busy": "2025-05-20T09:23:07.991626Z",
     "iopub.status.idle": "2025-05-20T09:23:08.004389Z",
     "shell.execute_reply": "2025-05-20T09:23:08.003153Z"
    },
    "papermill": {
     "duration": 0.018983,
     "end_time": "2025-05-20T09:23:08.006266",
     "exception": false,
     "start_time": "2025-05-20T09:23:07.987283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all model checkpoints from cfg.model_files and return initialized models\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    model_files = cfg.model_files\n",
    "\n",
    "    if not model_files:\n",
    "        print(f\"⚠️ No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "\n",
    "    print(f\"🔍 Found {len(model_files)} model file(s).\")\n",
    "\n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"📦 Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "\n",
    "            # checkpoint[\"cfg\"] は dict の場合を想定して再構築\n",
    "            if isinstance(checkpoint.get(\"cfg\"), dict):\n",
    "                cfg_dict = checkpoint[\"cfg\"]\n",
    "                cfg_temp = CFG()\n",
    "                for k, v in cfg_dict.items():\n",
    "                    setattr(cfg_temp, k, v)\n",
    "            else:\n",
    "                cfg_temp = checkpoint[\"cfg\"]  # すでにCFGならそのまま使う\n",
    "\n",
    "            cfg_temp.device = cfg.device  # 推論環境にあわせて上書き\n",
    "            cfg_temp.taxonomy_csv = cfg.taxonomy_csv \n",
    "\n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model {model_path}: {e}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "71bc210f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:08.014759Z",
     "iopub.status.busy": "2025-05-20T09:23:08.014388Z",
     "iopub.status.idle": "2025-05-20T09:23:08.027069Z",
     "shell.execute_reply": "2025-05-20T09:23:08.025973Z"
    },
    "papermill": {
     "duration": 0.019239,
     "end_time": "2025-05-20T09:23:08.028968",
     "exception": false,
     "start_time": "2025-05-20T09:23:08.009729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = sorted(Path(cfg.test_soundscapes).glob('*.ogg'))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "            executor.map(\n",
    "                predict_on_spectrogram,\n",
    "                test_files,\n",
    "                itertools.repeat(models),\n",
    "                itertools.repeat(cfg),\n",
    "                itertools.repeat(species_ids)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2008f010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T09:23:08.037101Z",
     "iopub.status.busy": "2025-05-20T09:23:08.036785Z",
     "iopub.status.idle": "2025-05-20T09:23:08.043050Z",
     "shell.execute_reply": "2025-05-20T09:23:08.042035Z"
    },
    "papermill": {
     "duration": 0.01205,
     "end_time": "2025-05-20T09:23:08.044640",
     "exception": false,
     "start_time": "2025-05-20T09:23:08.032590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "🔍 Found 1 model file(s).\n",
      "📦 Loading model: ../models/sedmodel_0857/model_fold0.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model usage: Single model\n",
      "Found 4 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230502_080500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission.csv\n",
      "Inference completed in 0.12 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "models = load_models(cfg, num_classes)\n",
    "\n",
    "if not models:\n",
    "    print(\"No models found! Please check model paths.\")\n",
    "    raise RuntimeError(\"No models loaded for inference.\")\n",
    "\n",
    "print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "smooth_submission(submission_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8717b697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230420_074000_5</td>\n",
       "      <td>0.019034</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.008441</td>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.013151</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.004325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H02_20230420_074000_10</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.010452</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.012963</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.013079</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.008929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H02_20230420_074000_15</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.014794</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230420_074000_20</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.011245</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>0.007894</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H02_20230420_074000_25</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.005349</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.009496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.015601</td>\n",
       "      <td>0.011264</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.011748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   row_id   1139490   1192948   1194042    126247   1346504  \\\n",
       "0   H02_20230420_074000_5  0.019034  0.014293  0.009067  0.003697  0.010040   \n",
       "1  H02_20230420_074000_10  0.005526  0.005460  0.003589  0.009103  0.005507   \n",
       "2  H02_20230420_074000_15  0.009316  0.016221  0.007963  0.011094  0.010349   \n",
       "3  H02_20230420_074000_20  0.010347  0.015713  0.004209  0.009343  0.012749   \n",
       "4  H02_20230420_074000_25  0.005793  0.005728  0.005349  0.011468  0.007462   \n",
       "\n",
       "     134933    135045   1462711   1462737  ...   yebfly1   yebsee1   yecspi2  \\\n",
       "0  0.009429  0.008441  0.013995  0.016443  ...  0.007549  0.007677  0.010365   \n",
       "1  0.011483  0.009386  0.006968  0.010645  ...  0.007237  0.005919  0.006159   \n",
       "2  0.008795  0.008718  0.010356  0.014014  ...  0.009298  0.007677  0.009697   \n",
       "3  0.009912  0.008312  0.011245  0.014436  ...  0.009790  0.007069  0.006696   \n",
       "4  0.011525  0.009423  0.006088  0.009496  ...  0.008974  0.007244  0.006177   \n",
       "\n",
       "    yectyr1   yehbla2   yehcar1   yelori1   yeofly1   yercac1    ywcpar  \n",
       "0  0.016833  0.007334  0.010245  0.009943  0.013151  0.020206  0.004325  \n",
       "1  0.010452  0.005051  0.012963  0.006981  0.013079  0.007854  0.008929  \n",
       "2  0.014609  0.006194  0.015556  0.010150  0.014794  0.012863  0.011697  \n",
       "3  0.016567  0.007894  0.015836  0.011767  0.014757  0.012819  0.010654  \n",
       "4  0.018225  0.007333  0.015601  0.011264  0.013051  0.012356  0.011748  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83143f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7130272,
     "sourceId": 11719803,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.275304,
   "end_time": "2025-05-20T09:23:45.886660",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-20T09:22:39.611356",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
